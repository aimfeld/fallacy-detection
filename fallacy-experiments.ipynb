{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:28:48.722607Z",
     "start_time": "2024-10-29T22:28:47.700372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from src.llms import get_llms, init_langchain, LLM\n",
    "from src.experiment import get_fallacy_df, save_fallacy_df, run_experiment, get_classification_prompt_template\n",
    "from src.tuning import TuningSet\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "init_langchain()"
   ],
   "id": "801f0c9f57db9f3c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fallacy Experiments",
   "id": "1d8205f73aece599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fallacy Identification",
   "id": "2f70cdc3be871171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment 1: Fallacy Identification with zero-shot Prompt",
   "id": "15c17be359103d5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:19:43.916302Z",
     "start_time": "2024-10-29T17:19:43.877680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e1_filename = 'data/fallacies_e1.csv'\n",
    "df_fallacies_e1 = get_fallacy_df(e1_filename)\n",
    "df_fallacies_e1.head()"
   ],
   "id": "86cb1cc7b84b2c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-29 18:19:43] Loaded existing fallacy dataframe from data/fallacies_e1.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory gpt_4o_response  \\\n",
       "0  Argument from Silence      1  informal  insufficiency             No.   \n",
       "1  Argument from Silence      1  informal  insufficiency             No.   \n",
       "2  Argument from Silence      1  informal  insufficiency             No.   \n",
       "3  Argument from Silence      1  informal  insufficiency             No.   \n",
       "4  Argument from Silence      1  informal  insufficiency             No.   \n",
       "\n",
       "  gpt_4_response gpt_4o_mini_response claude_3_5_sonnet_response  \\\n",
       "0             No                  No.                         No   \n",
       "1             No                  No.                         No   \n",
       "2             No                  No.                         No   \n",
       "3             No                  No.                         No   \n",
       "4             No                  No.                         No   \n",
       "\n",
       "  claude_3_opus_response claude_3_haiku_response gemini_1_5_pro_response  \\\n",
       "0                    No.                     No.                      No   \n",
       "1                    No.                     No.                      No   \n",
       "2                    No.                     No.                      No   \n",
       "3                    No.                     No.                     Yes   \n",
       "4                    No.                     No.                      No   \n",
       "\n",
       "  gemini_1_5_flash_response gemini_1_5_flash_8b_response  \\\n",
       "0                        No                           No   \n",
       "1                        No                           No   \n",
       "2                        No                           No   \n",
       "3                        No                           No   \n",
       "4                        No                           No   \n",
       "\n",
       "  llama_3_1_70b_response llama_3_1_8b_response  \n",
       "0                     No                    No  \n",
       "1                    No.                    No  \n",
       "2                     No                    No  \n",
       "3                     No                    No  \n",
       "4                     No                    No  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>claude_3_opus_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "      <th>llama_3_1_8b_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T17:49:46.045225Z",
     "start_time": "2024-10-29T17:19:55.746695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.LLAMA_3_1_70B])\n",
    "\n",
    "prompt_template_e1 = \"\"\"Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
    "[step]\"\"\"\n",
    "run_experiment(df_fallacies_e1, e1_filename, prompt_template_e1, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e1, e1_filename)"
   ],
   "id": "b99e69538350883",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n",
      "[2024-10-29 18:21:04] Processed 100 responses for LLM llama_3_1_70b (index=1578).\n",
      "[2024-10-29 18:22:05] Processed 200 responses for LLM llama_3_1_70b (index=1678).\n",
      "[2024-10-29 18:23:00] Processed 300 responses for LLM llama_3_1_70b (index=1778).\n",
      "[2024-10-29 18:23:50] Processed 400 responses for LLM llama_3_1_70b (index=1878).\n",
      "[2024-10-29 18:24:48] Processed 500 responses for LLM llama_3_1_70b (index=1978).\n",
      "[2024-10-29 18:25:43] Processed 600 responses for LLM llama_3_1_70b (index=2078).\n",
      "[2024-10-29 18:26:48] Processed 700 responses for LLM llama_3_1_70b (index=2178).\n",
      "[2024-10-29 18:27:58] Processed 800 responses for LLM llama_3_1_70b (index=2278).\n",
      "[2024-10-29 18:28:42] Processed 900 responses for LLM llama_3_1_70b (index=2378).\n",
      "[2024-10-29 18:29:52] Processed 1000 responses for LLM llama_3_1_70b (index=2478).\n",
      "[2024-10-29 18:30:44] Processed 1100 responses for LLM llama_3_1_70b (index=2578).\n",
      "[2024-10-29 18:31:28] Processed 1200 responses for LLM llama_3_1_70b (index=2678).\n",
      "[2024-10-29 18:32:25] Processed 1300 responses for LLM llama_3_1_70b (index=2778).\n",
      "[2024-10-29 18:33:20] Processed 1400 responses for LLM llama_3_1_70b (index=2878).\n",
      "[2024-10-29 18:34:09] Processed 1500 responses for LLM llama_3_1_70b (index=2978).\n",
      "[2024-10-29 18:34:56] Processed 1600 responses for LLM llama_3_1_70b (index=3078).\n",
      "[2024-10-29 18:35:47] Processed 1700 responses for LLM llama_3_1_70b (index=3178).\n",
      "[2024-10-29 18:36:57] Processed 1800 responses for LLM llama_3_1_70b (index=3278).\n",
      "[2024-10-29 18:38:08] Processed 1900 responses for LLM llama_3_1_70b (index=3378).\n",
      "[2024-10-29 18:39:03] Processed 2000 responses for LLM llama_3_1_70b (index=3478).\n",
      "[2024-10-29 18:39:54] Processed 2100 responses for LLM llama_3_1_70b (index=3578).\n",
      "[2024-10-29 18:40:43] Processed 2200 responses for LLM llama_3_1_70b (index=3678).\n",
      "[2024-10-29 18:41:39] Processed 2300 responses for LLM llama_3_1_70b (index=3778).\n",
      "[2024-10-29 18:42:34] Processed 2400 responses for LLM llama_3_1_70b (index=3878).\n",
      "[2024-10-29 18:43:30] Processed 2500 responses for LLM llama_3_1_70b (index=3978).\n",
      "[2024-10-29 18:44:27] Processed 2600 responses for LLM llama_3_1_70b (index=4078).\n",
      "[2024-10-29 18:45:12] Processed 2700 responses for LLM llama_3_1_70b (index=4178).\n",
      "[2024-10-29 18:46:10] Processed 2800 responses for LLM llama_3_1_70b (index=4278).\n",
      "[2024-10-29 18:47:15] Processed 2900 responses for LLM llama_3_1_70b (index=4378).\n",
      "[2024-10-29 18:48:14] Processed 3000 responses for LLM llama_3_1_70b (index=4478).\n",
      "[2024-10-29 18:49:05] Processed 3100 responses for LLM llama_3_1_70b (index=4578).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment 2: Fallacy Identification with few-shot Prompt",
   "id": "60615736cb645e61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:05:04.839433Z",
     "start_time": "2024-10-29T22:05:04.658217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e2_filename = 'data/fallacies_e2.csv'\n",
    "df_fallacies_e2 = get_fallacy_df(e2_filename)\n",
    "df_fallacies_e2.head()"
   ],
   "id": "d971e94c8e05d198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-29 23:05:04] Loaded existing fallacy dataframe from data/fallacies_e2.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory gpt_4o_response  \\\n",
       "0  Argument from Silence      1  informal  insufficiency             No.   \n",
       "1  Argument from Silence      1  informal  insufficiency             No.   \n",
       "2  Argument from Silence      1  informal  insufficiency             No.   \n",
       "3  Argument from Silence      1  informal  insufficiency             No.   \n",
       "4  Argument from Silence      1  informal  insufficiency             No.   \n",
       "\n",
       "  claude_3_5_sonnet_response gemini_1_5_pro_response gpt_4o_mini_response  \\\n",
       "0                        No.                     No.                  No.   \n",
       "1                        No.                     No.                  No.   \n",
       "2                        No.                     No.                  No.   \n",
       "3                        No.                    Yes.                  No.   \n",
       "4                        No.                     No.                  No.   \n",
       "\n",
       "  claude_3_haiku_response gemini_1_5_flash_response  \\\n",
       "0                     No.                        No   \n",
       "1                     No.                        No   \n",
       "2                     No.                        No   \n",
       "3                     No.                        No   \n",
       "4                     No.                        No   \n",
       "\n",
       "  gemini_1_5_flash_8b_response llama_3_1_70b_response llama_3_1_8b_response  \n",
       "0                           No                     No                   No.  \n",
       "1                           No                    No.                   No.  \n",
       "2                           No                     No                   No.  \n",
       "3                           No                    No.                   No.  \n",
       "4                           No                    No.                   No.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "      <th>llama_3_1_8b_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:05:13.719466Z",
     "start_time": "2024-10-29T22:05:06.688601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# llms = get_llms([LLM.GPT_4O, LLM.CLAUDE_3_5_SONNET, LLM.GEMINI_1_5_PRO])\n",
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "prompt_template_e2 = \"\"\"Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
    "Since if it's raining then the streets are wet and it's raining now, therefore, the streets are wet.\n",
    "Yes.\n",
    "Since I found a shell on the beach and this shell was beautifully shaped and colored, therefore, all shells are beautifully shaped and colored.\n",
    "No.\n",
    "Since I am at home or I am in the city and I am at home, therefore, I am not in the city.\n",
    "No.\n",
    "Since heavy snowfall often leads to traffic jams and traffic jams cause delays, therefore, heavy snowfall can lead to delays.\n",
    "Yes.\n",
    "[step]\"\"\"\n",
    "\n",
    "run_experiment(df_fallacies_e2, e2_filename, prompt_template_e2, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e2, e2_filename)\n"
   ],
   "id": "b24032214d1174f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment 3: Fallacy Identification with chain-of-thought Prompt",
   "id": "b7f0494cfd3f1840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:24:34.019143Z",
     "start_time": "2024-10-29T22:24:33.740425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e3_filename = 'data/fallacies_e3.csv'\n",
    "df_fallacies_e3 = get_fallacy_df(e3_filename)\n",
    "df_fallacies_e3.head()"
   ],
   "id": "381c9adbd379caa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-29 23:24:34] Loaded existing fallacy dataframe from data/fallacies_e3.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory  \\\n",
       "0  Argument from Silence      1  informal  insufficiency   \n",
       "1  Argument from Silence      1  informal  insufficiency   \n",
       "2  Argument from Silence      1  informal  insufficiency   \n",
       "3  Argument from Silence      1  informal  insufficiency   \n",
       "4  Argument from Silence      1  informal  insufficiency   \n",
       "\n",
       "                                     gpt_4o_response  \\\n",
       "0  No. The reasoning step is not necessarily corr...   \n",
       "1  No. The reasoning step is not correct. Bob's s...   \n",
       "2  No. The reasoning step is not correct. The con...   \n",
       "3  No. The reasoning step is not necessarily corr...   \n",
       "4  No. The reasoning step is not correct. The fac...   \n",
       "\n",
       "                          claude_3_5_sonnet_response  \\\n",
       "0  Let's think through this step-by-step:  1. Joh...   \n",
       "1  Let's think through this step by step:  1. Ali...   \n",
       "2  Let's think through this step-by-step:  1. Ali...   \n",
       "3  Let's think through this step-by-step:  1. The...   \n",
       "4  Let's think through this step-by-step:  1. Joh...   \n",
       "\n",
       "                             gemini_1_5_pro_response gpt_4o_mini_response  \\\n",
       "0  No.  Silence is not necessarily an admission o...                  No.   \n",
       "1  No.  Silence could indicate a lack of knowledg...                  No.   \n",
       "2  No.  Bob's silence doesn't confirm Alice's cla...                  No.   \n",
       "3  No.  While Tom's experience is valuable, the a...                  No.   \n",
       "4  No.  Silence does not imply guilt or fear.  Ma...                  No.   \n",
       "\n",
       "                             claude_3_haiku_response  \\\n",
       "0  Let's analyze this step-by-step:  1. John aske...   \n",
       "1  Let's analyze this step-by-step:  1. Alice ask...   \n",
       "2  Let's analyze this step-by-step:  1. Alice cla...   \n",
       "3  Let's analyze this step-by-step:  1. Tom, a se...   \n",
       "4  Let's analyze this step-by-step:  1. John accu...   \n",
       "\n",
       "                           gemini_1_5_flash_response  \\\n",
       "0  No.  Silence doesn't necessarily mean guilt or...   \n",
       "1  No.  Silence doesn't imply knowledge.  Bob's s...   \n",
       "2  No.  Bob's silence doesn't provide evidence su...   \n",
       "3  No.  The reasoning commits an appeal to author...   \n",
       "4  No.  Silence does not equal guilt or fear.  Ma...   \n",
       "\n",
       "                        gemini_1_5_flash_8b_response  \n",
       "0  No.  Silence doesn't necessarily mean agreemen...  \n",
       "1  No.  Silence does not equate to knowledge.  Bo...  \n",
       "2  No.  Alice's claim, even if true, and Bob's si...  \n",
       "3  No.  Just because one person says something an...  \n",
       "4  No.  Silence in the face of an accusation does...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not necessarily corr...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Joh...</td>\n",
       "      <td>No.  Silence is not necessarily an admission o...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. John aske...</td>\n",
       "      <td>No.  Silence doesn't necessarily mean guilt or...</td>\n",
       "      <td>No.  Silence doesn't necessarily mean agreemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. Bob's s...</td>\n",
       "      <td>Let's think through this step by step:  1. Ali...</td>\n",
       "      <td>No.  Silence could indicate a lack of knowledg...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Alice ask...</td>\n",
       "      <td>No.  Silence doesn't imply knowledge.  Bob's s...</td>\n",
       "      <td>No.  Silence does not equate to knowledge.  Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. The con...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Ali...</td>\n",
       "      <td>No.  Bob's silence doesn't confirm Alice's cla...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Alice cla...</td>\n",
       "      <td>No.  Bob's silence doesn't provide evidence su...</td>\n",
       "      <td>No.  Alice's claim, even if true, and Bob's si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not necessarily corr...</td>\n",
       "      <td>Let's think through this step-by-step:  1. The...</td>\n",
       "      <td>No.  While Tom's experience is valuable, the a...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Tom, a se...</td>\n",
       "      <td>No.  The reasoning commits an appeal to author...</td>\n",
       "      <td>No.  Just because one person says something an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. The fac...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Joh...</td>\n",
       "      <td>No.  Silence does not imply guilt or fear.  Ma...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. John accu...</td>\n",
       "      <td>No.  Silence does not equal guilt or fear.  Ma...</td>\n",
       "      <td>No.  Silence in the face of an accusation does...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:28:39.589553Z",
     "start_time": "2024-10-29T22:25:27.728971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# llms = get_llms([LLM.GPT_4O, LLM.CLAUDE_3_5_SONNET, LLM.GEMINI_1_5_PRO])\n",
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "prompt_template_e3 = \"\"\"Is the following reasoning step correct?\n",
    "Let's think step by step and then answer \"Yes\" or \"No\".\n",
    "[step]\"\"\"\n",
    "\n",
    "run_experiment(df_fallacies_e3, e3_filename, prompt_template_e3, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e3, e3_filename)\n"
   ],
   "id": "a0c8bd26ad1fafd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m llms \u001B[38;5;241m=\u001B[39m get_llms([LLM\u001B[38;5;241m.\u001B[39mLLAMA_3_1_70B, LLM\u001B[38;5;241m.\u001B[39mLLAMA_3_1_8B])\n\u001B[1;32m      4\u001B[0m prompt_template_e3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mIs the following reasoning step correct?\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124mLet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms think step by step and then answer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYes\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m[step]\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m----> 8\u001B[0m run_experiment(df_fallacies_e3, e3_filename, prompt_template_e3, llms, sleep_seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     10\u001B[0m save_fallacy_df(df_fallacies_e3, e3_filename)\n",
      "File \u001B[0;32m~/HSLU/Thesis/fallacy-detection/src/experiment.py:64\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(df_fallacies, filename, prompt_template, llms, keep_existing_responses, sleep_seconds, log_responses)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;66;03m# log(f\"Prompting LLM {llm.key}: {prompt}\")\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m     response: AIMessage \u001B[38;5;241m=\u001B[39m llm_chat\u001B[38;5;241m.\u001B[39minvoke(prompt)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m log_responses:\n\u001B[1;32m     66\u001B[0m         log(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse from LLM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mllm\u001B[38;5;241m.\u001B[39mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (index=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    285\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 286\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    287\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    288\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    289\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    290\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    291\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    292\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    293\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    294\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    295\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    647\u001B[0m ]\n\u001B[1;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 633\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    634\u001B[0m                 m,\n\u001B[1;32m    635\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    636\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    637\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    638\u001B[0m             )\n\u001B[1;32m    639\u001B[0m         )\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    852\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    853\u001B[0m         )\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_huggingface/chat_models/huggingface.py:370\u001B[0m, in \u001B[0;36mChatHuggingFace._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m _is_huggingface_endpoint(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm):\n\u001B[1;32m    369\u001B[0m     message_dicts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m--> 370\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mchat_completion(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    371\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(answer)\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:837\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    833\u001B[0m \u001B[38;5;66;03m# `model` is sent in the payload. Not used by the server but can be useful for debugging/routing.\u001B[39;00m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;66;03m# If it's a ID on the Hub => use it. Otherwise, we use a random string.\u001B[39;00m\n\u001B[1;32m    835\u001B[0m model_id \u001B[38;5;241m=\u001B[39m model \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_url \u001B[38;5;129;01mand\u001B[39;00m model\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 837\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    838\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_url,\n\u001B[1;32m    839\u001B[0m     json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    840\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m    841\u001B[0m         messages\u001B[38;5;241m=\u001B[39mmessages,\n\u001B[1;32m    842\u001B[0m         frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m    843\u001B[0m         logit_bias\u001B[38;5;241m=\u001B[39mlogit_bias,\n\u001B[1;32m    844\u001B[0m         logprobs\u001B[38;5;241m=\u001B[39mlogprobs,\n\u001B[1;32m    845\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    846\u001B[0m         n\u001B[38;5;241m=\u001B[39mn,\n\u001B[1;32m    847\u001B[0m         presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m    848\u001B[0m         response_format\u001B[38;5;241m=\u001B[39mresponse_format,\n\u001B[1;32m    849\u001B[0m         seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    850\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    851\u001B[0m         temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m    852\u001B[0m         tool_choice\u001B[38;5;241m=\u001B[39mtool_choice,\n\u001B[1;32m    853\u001B[0m         tool_prompt\u001B[38;5;241m=\u001B[39mtool_prompt,\n\u001B[1;32m    854\u001B[0m         tools\u001B[38;5;241m=\u001B[39mtools,\n\u001B[1;32m    855\u001B[0m         top_logprobs\u001B[38;5;241m=\u001B[39mtop_logprobs,\n\u001B[1;32m    856\u001B[0m         top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m    857\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    858\u001B[0m     ),\n\u001B[1;32m    859\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    860\u001B[0m )\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _stream_chat_completion_response(data)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:289\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_as_binary(data) \u001B[38;5;28;01mas\u001B[39;00m data_as_binary:\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 289\u001B[0m         response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    290\u001B[0m             url,\n\u001B[1;32m    291\u001B[0m             json\u001B[38;5;241m=\u001B[39mjson,\n\u001B[1;32m    292\u001B[0m             data\u001B[38;5;241m=\u001B[39mdata_as_binary,\n\u001B[1;32m    293\u001B[0m             headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    294\u001B[0m             cookies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcookies,\n\u001B[1;32m    295\u001B[0m             timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout,\n\u001B[1;32m    296\u001B[0m             stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    297\u001B[0m             proxies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproxies,\n\u001B[1;32m    298\u001B[0m         )\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m    300\u001B[0m         \u001B[38;5;66;03m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001B[39;00m\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/requests/sessions.py:637\u001B[0m, in \u001B[0;36mSession.post\u001B[0;34m(self, url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    627\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    628\u001B[0m \n\u001B[1;32m    629\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, data\u001B[38;5;241m=\u001B[39mdata, json\u001B[38;5;241m=\u001B[39mjson, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:66\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     68\u001B[0m     request_id \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[1;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[1;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[1;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    679\u001B[0m     )\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[1;32m    790\u001B[0m     conn,\n\u001B[1;32m    791\u001B[0m     method,\n\u001B[1;32m    792\u001B[0m     url,\n\u001B[1;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[1;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[1;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[1;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[1;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[1;32m    802\u001B[0m )\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/urllib3/connection.py:507\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 507\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/http/client.py:1395\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1393\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1394\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m         response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[1;32m   1396\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/http/client.py:325\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 325\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/http/client.py:286\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 286\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/socket.py:718\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 718\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    719\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    720\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/ssl.py:1314\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1310\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1311\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1312\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1313\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/ssl.py:1166\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1166\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1168\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fallacy Classification",
   "id": "127c2d47d3dc8918"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment 4: Fallacy Classification with zero-shot Prompt",
   "id": "54643504afccdcb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:29:00.435311Z",
     "start_time": "2024-10-29T22:29:00.260542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e4_filename = 'data/fallacies_e4.csv'\n",
    "df_fallacies_e4 = get_fallacy_df(e4_filename, only_incorrect=True)\n",
    "\n",
    "df_fallacies_e4.head()"
   ],
   "id": "5de2807d037c5b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-29 23:29:00] Loaded existing fallacy dataframe from data/fallacies_e4.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory  \\\n",
       "0  Argument from Silence      1  informal  insufficiency   \n",
       "1  Argument from Silence      1  informal  insufficiency   \n",
       "2  Argument from Silence      1  informal  insufficiency   \n",
       "3  Argument from Silence      1  informal  insufficiency   \n",
       "4  Argument from Silence      1  informal  insufficiency   \n",
       "\n",
       "         gpt_4o_response            gpt_4_response      gpt_4o_mini_response  \\\n",
       "0  Argument from Silence     Argument from Silence  Affirming the Consequent   \n",
       "1  Argument from Silence     Argument from Silence  Affirming the Consequent   \n",
       "2  Argument from Silence     Argument from Silence     Argument from Silence   \n",
       "3  Argument from Silence  (79) Appeal to Authority       Appeal to Authority   \n",
       "4  Argument from Silence     Argument from Silence     Argument from Silence   \n",
       "\n",
       "  claude_3_5_sonnet_response claude_3_haiku_response gemini_1_5_pro_response  \\\n",
       "0      Argument from Silence  Denying the Antecedent   Argument from Silence   \n",
       "1      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "2      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "3        Appeal to Authority     Appeal to Authority     Appeal to Authority   \n",
       "4      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "\n",
       "  gemini_1_5_flash_8b_response    o1_preview_response llama_3_1_70b_response  \\\n",
       "0        Argument from Silence  Argument from Silence  Argument from Silence   \n",
       "1        Argument from Silence  Argument from Silence  Argument from Silence   \n",
       "2        Argument from Silence  Argument from Silence  Argument from Silence   \n",
       "3          Appeal to Authority  Argument from Silence  (205) Proof Surrogate   \n",
       "4        Argument from Silence  Argument from Silence  Argument from Silence   \n",
       "\n",
       "            llama_3_1_8b_response  \n",
       "0       Affirming the Consequent.  \n",
       "1          Argument from Silence.  \n",
       "2          Argument from Silence.  \n",
       "3  Argument from False Authority.  \n",
       "4         (210) False Attribution  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>o1_preview_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "      <th>llama_3_1_8b_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Affirming the Consequent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Denying the Antecedent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Affirming the Consequent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Affirming the Consequent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>(79) Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>(205) Proof Surrogate</td>\n",
       "      <td>Argument from False Authority.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>(210) False Attribution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:29:01.987585Z",
     "start_time": "2024-10-29T22:29:01.966678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template_e4 = get_classification_prompt_template()\n",
    "print(prompt_template_e4)"
   ],
   "id": "d2a36cf979639663",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a logical fallacy classifier. Given an incorrect reasoning step, your task is to identify its type of fallacy.\n",
      "Answer by choosing one of these fallacies:\n",
      "(1) Affirming the Consequent\n",
      "(2) Denying the Antecedent\n",
      "(3) Negating Antecedent and Consequent\n",
      "(4) Commutation of Conditionals\n",
      "(5) Affirming a Disjunct\n",
      "(6) Denying a Conjunct\n",
      "(7) Fallacy of the Undistributed Middle\n",
      "(8) Exclusive Premises\n",
      "(9) Fallacy of Four Terms\n",
      "(10) Illicit Substitution of Identicals\n",
      "(11) Illicit Minor\n",
      "(12) Illicit Major\n",
      "(13) Negative Conclusion from Affirmative Premises\n",
      "(14) Affirmative Conclusion from a Negative Premise\n",
      "(15) False Conversion\n",
      "(16) Unwarranted Contrast\n",
      "(17) Quantifier Shift Fallacy\n",
      "(18) Existential Fallacy\n",
      "(19) Fallacy of Every and All\n",
      "(20) Illicit Contraposition\n",
      "(21) Gamblers Fallacy\n",
      "(22) Hot Hand Fallacy\n",
      "(23) Conjunction Fallacy\n",
      "(24) Disjunction Fallacy\n",
      "(25) Argument of the Beard\n",
      "(26) Appeal to Extremes\n",
      "(27) Type Token Fallacy\n",
      "(28) Use Mention Error\n",
      "(29) Reification\n",
      "(30) Fake Precision\n",
      "(31) No True Scotsman\n",
      "(32) Contextomy\n",
      "(33) Stolen Concept Fallacy\n",
      "(34) Anthropomorphism\n",
      "(35) Accent Fallacy\n",
      "(36) Ambiguity Fallacy\n",
      "(37) Alphabet Soup\n",
      "(38) Equivocation\n",
      "(39) Modal Scope Fallacy\n",
      "(40) Inconsistency\n",
      "(41) Conflicting Conditions\n",
      "(42) Kettle Logic\n",
      "(43) Political Correctness Fallacy\n",
      "(44) Appeal to Complexity\n",
      "(45) Statement of Conversion\n",
      "(46) Appeal to the Moon\n",
      "(47) Quantum Physics Fallacy\n",
      "(48) fact to fiction fallacy\n",
      "(49) Non Sequitur\n",
      "(50) Inflation of Conflict\n",
      "(51) Argument by Fast Talking\n",
      "(52) Appeal to Intuition\n",
      "(53) Appeal to Closure\n",
      "(54) Appeal to Definition\n",
      "(55) Spiritual Fallacy\n",
      "(56) gish gallop\n",
      "(57) Denying the Correlative\n",
      "(58) Red Herring\n",
      "(59) Strawman Fallacy\n",
      "(60) Avoiding the Issue\n",
      "(61) Logic Chopping\n",
      "(62) Meaningless Question\n",
      "(63) Failure to Elucidate\n",
      "(64) Argument by Gibberish\n",
      "(65) Hypnotic Bait and Switch\n",
      "(66) Traitorous Critic Fallacy\n",
      "(67) Having Your Cake\n",
      "(68) Appeal to Common Belief\n",
      "(69) Appeal to Popularity\n",
      "(70) Appeal to Common Sense\n",
      "(71) Appeal to Common Folk\n",
      "(72) Appeal to Trust\n",
      "(73) Argument from Age\n",
      "(74) Appeal to Heaven\n",
      "(75) Appeal to Tradition\n",
      "(76) Etymological Fallacy\n",
      "(77) Genetic Fallacy\n",
      "(78) Appeal to Celebrity\n",
      "(79) Appeal to Authority\n",
      "(80) Appeal to False Authority\n",
      "(81) Argument from False Authority\n",
      "(82) Blind Authority Fallacy\n",
      "(83) Argument by Personal Charm\n",
      "(84) Argument to the Purse\n",
      "(85) Ad Hominem Circumstantial\n",
      "(86) Gadarene Swine Fallacy\n",
      "(87) Ad Hominem Tu quoque\n",
      "(88) Bulverism\n",
      "(89) Righteousness Fallacy\n",
      "(90) Self Righteousness Fallacy\n",
      "(91) Reductio ad Hitlerum\n",
      "(92) Ad Hominem Guilt by Association\n",
      "(93) Identity Fallacy\n",
      "(94) Appeal to Stupidity\n",
      "(95) Ad Hominem Abusive\n",
      "(96) Ad Fidentia\n",
      "(97) appeal to loyalty\n",
      "(98) Appeal to Accomplishment\n",
      "(99) Scapegoating\n",
      "(100) Fallacy of Opposition\n",
      "(101) Proof by Intimidation\n",
      "(102) Poisoning the Well\n",
      "(103) Wishful Thinking\n",
      "(104) Appeal to Faith\n",
      "(105) Notable Effort\n",
      "(106) Prejudicial Language\n",
      "(107) Special Pleading\n",
      "(108) If By Whiskey\n",
      "(109) Overextended Outrage\n",
      "(110) Appeal to Ridicule\n",
      "(111) Argument by Emotive Language\n",
      "(112) Style Over Substance\n",
      "(113) Appeal to Anger\n",
      "(114) Appeal to Pity\n",
      "(115) Appeal to Emotion\n",
      "(116) Appeal to Flattery\n",
      "(117) Appeal to Spite\n",
      "(118) pragmatic fallacy\n",
      "(119) Appeal to Force\n",
      "(120) Appeal to Fear\n",
      "(121) Fallacy of Composition\n",
      "(122) Fallacy of Division\n",
      "(123) Stereotyping the fallacy\n",
      "(124) Ecological Fallacy\n",
      "(125) Oversimplified Cause Fallacy\n",
      "(126) Accident Fallacy\n",
      "(127) mcnamara fallacy\n",
      "(128) Overwhelming Exception\n",
      "(129) Reductio ad Absurdum\n",
      "(130) Nirvana Fallacy\n",
      "(131) Relative Privation\n",
      "(132) imposter fallacy\n",
      "(133) Misleading Vividness\n",
      "(134) Appeal to Possibility\n",
      "(135) Rights To Ought Fallacy\n",
      "(136) Psychogenetic Fallacy\n",
      "(137) Weak Analogy\n",
      "(138) Extended Analogy\n",
      "(139) Appeal to Equality\n",
      "(140) False Equivalence\n",
      "(141) Galileo Fallacy\n",
      "(142) Post Designation\n",
      "(143) Just In Case Fallacy\n",
      "(144) Selective Attention\n",
      "(145) nutpicking fallacy\n",
      "(146) Biased Sample Fallacy\n",
      "(147) Survivorship Fallacy\n",
      "(148) Spotlight Fallacy\n",
      "(149) Hasty Generalization\n",
      "(150) Incomplete Comparison\n",
      "(151) Texas Sharpshooter Fallacy\n",
      "(152) Faulty Comparison\n",
      "(153) Base Rate Fallacy\n",
      "(154) Least Plausible Hypothesis\n",
      "(155) Far Fetched Hypothesis\n",
      "(156) Cherry Picking\n",
      "(157) Argument by Selective Reading\n",
      "(158) deceptive sharing\n",
      "(159) Multiple Comparisons Fallacy\n",
      "(160) Magical Thinking\n",
      "(161) Slippery Slope\n",
      "(162) Sunk Cost Fallacy\n",
      "(163) Jumping to Conclusions\n",
      "(164) Argument from Silence\n",
      "(165) Argument from Hearsay\n",
      "(166) Anonymous Authority\n",
      "(167) Insignificant Cause\n",
      "(168) Just Because Fallacy\n",
      "(169) Appeal to the Law\n",
      "(170) Appeal to Normality\n",
      "(171) False Effect\n",
      "(172) Appeal to Consequences\n",
      "(173) Retrogressive Causation\n",
      "(174) Confusing Currently Unexplained with Unexplainable\n",
      "(175) Appeal to Desperation\n",
      "(176) Regression Fallacy\n",
      "(177) Causal Reductionism\n",
      "(178) Questionable Cause\n",
      "(179) Hedging\n",
      "(180) Circular Definition\n",
      "(181) Homunculus Fallacy\n",
      "(182) Circular Reasoning\n",
      "(183) Tokenism\n",
      "(184) Appeal to Novelty\n",
      "(185) Two Wrongs Make a Right\n",
      "(186) Appeal to Nature\n",
      "(187) Naturalistic Fallacy\n",
      "(188) Moralistic Fallacy\n",
      "(189) Suppressed Correlative\n",
      "(190) Historians Fallacy\n",
      "(191) Willed Ignorance\n",
      "(192) Appeal to Coincidence\n",
      "(193) Argument from Incredulity\n",
      "(194) Argument by Pigheadedness\n",
      "(195) Argument by Repetition\n",
      "(196) Definist Fallacy\n",
      "(197) Limited Scope\n",
      "(198) Moving the Goalposts\n",
      "(199) Argument from Fallacy\n",
      "(200) False Dilemma\n",
      "(201) Argument from Ignorance\n",
      "(202) Alternative Advance\n",
      "(203) Shifting of the Burden of Proof\n",
      "(204) Proving Non Existence\n",
      "(205) Proof Surrogate\n",
      "(206) Rationalization\n",
      "(207) Spin Doctoring\n",
      "(208) Lying with Statistics\n",
      "(209) Ad Hoc Rescue\n",
      "(210) False Attribution\n",
      "(211) Amazing Familiarity\n",
      "(212) Ludic Fallacy\n",
      "(213) Missing Data Fallacy\n",
      "(214) Begging the Question\n",
      "(215) Complex Question Fallacy\n",
      "(216) Package Deal Fallacy\n",
      "(217) Subjectivist Fallacy\n",
      "(218) Distinction Without a Difference\n",
      "(219) Hypothesis Contrary to Fact\n",
      "(220) Shoehorning\n",
      "(221) Appeal to Self evident Truth\n",
      "(222) Subverted Support\n",
      "(223) Double Standard\n",
      "(224) Fantasy Projection\n",
      "(225) Argument to Moderation\n",
      "(226) Broken Window Fallacy\n",
      "(227) Self Sealing Argument\n",
      "(228) Unfalsifiability\n",
      "(229) Conspiracy Theory\n",
      "(230) Confusing an Explanation with an Excuse\n",
      "(231) Limited Depth\n",
      "(232) Alleged Certainty\n",
      "You should only answer the name of the fallacy.\n",
      "What type of fallacy does the following reasoning step belong to?\n",
      "[step]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T23:04:34.301021Z",
     "start_time": "2024-10-29T22:29:05.356489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "run_experiment(df_fallacies_e4, e4_filename, prompt_template_e4, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e4, e4_filename)"
   ],
   "id": "bc275feb959a3213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/aimfeld/.cache/huggingface/token\n",
      "Login successful\n",
      "[2024-10-29 23:30:51] Processed 100 responses for LLM llama_3_1_8b (index=612).\n",
      "[2024-10-29 23:32:50] Processed 200 responses for LLM llama_3_1_8b (index=712).\n",
      "[2024-10-29 23:34:45] Processed 300 responses for LLM llama_3_1_8b (index=812).\n",
      "[2024-10-29 23:36:37] Processed 400 responses for LLM llama_3_1_8b (index=912).\n",
      "[2024-10-29 23:38:35] Processed 500 responses for LLM llama_3_1_8b (index=1012).\n",
      "[2024-10-29 23:40:33] Processed 600 responses for LLM llama_3_1_8b (index=1112).\n",
      "[2024-10-29 23:42:36] Processed 700 responses for LLM llama_3_1_8b (index=1212).\n",
      "[2024-10-29 23:44:40] Processed 800 responses for LLM llama_3_1_8b (index=1312).\n",
      "[2024-10-29 23:46:37] Processed 900 responses for LLM llama_3_1_8b (index=1412).\n",
      "[2024-10-29 23:48:37] Processed 1000 responses for LLM llama_3_1_8b (index=1512).\n",
      "[2024-10-29 23:50:32] Processed 1100 responses for LLM llama_3_1_8b (index=1612).\n",
      "[2024-10-29 23:52:33] Processed 1200 responses for LLM llama_3_1_8b (index=1712).\n",
      "[2024-10-29 23:54:32] Processed 1300 responses for LLM llama_3_1_8b (index=1812).\n",
      "[2024-10-29 23:56:24] Processed 1400 responses for LLM llama_3_1_8b (index=1912).\n",
      "[2024-10-29 23:58:22] Processed 1500 responses for LLM llama_3_1_8b (index=2012).\n",
      "[2024-10-30 00:00:27] Processed 1600 responses for LLM llama_3_1_8b (index=2112).\n",
      "[2024-10-30 00:02:24] Processed 1700 responses for LLM llama_3_1_8b (index=2212).\n",
      "[2024-10-30 00:04:25] Processed 1800 responses for LLM llama_3_1_8b (index=2312).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment 5: Fallacy Classification with Fine-Tuning",
   "id": "116fa42a73fc8e66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:41:15.475569Z",
     "start_time": "2024-10-28T13:41:15.449984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e5_filename = 'data/fallacies_e5.csv'\n",
    "df_fallacies_e5 = get_fallacy_df(e5_filename, only_incorrect=True)\n",
    "\n",
    "# Select only test set\n",
    "df_fallacies_e5 = df_fallacies_e5[df_fallacies_e5['tuning'] == TuningSet.TEST.value]"
   ],
   "id": "9baec47ce0cd3a60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-28 14:41:15] Loaded existing fallacy dataframe from data/fallacies_e5.csv.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T14:06:14.968149Z",
     "start_time": "2024-10-28T13:51:46.025181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template_e5 = get_classification_prompt_template()\n",
    "\n",
    "llms = get_llms([LLM.GPT_4O_MINI_TUNED])\n",
    "\n",
    "run_experiment(df_fallacies_e5, e5_filename, prompt_template_e5, llms)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e5, e5_filename)"
   ],
   "id": "bfba5a381e2560bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-28 14:52:57] Processed 100 responses for LLM gpt_4o_mini_tuned_v1 (index=199).\n",
      "[2024-10-28 14:54:09] Processed 200 responses for LLM gpt_4o_mini_tuned_v1 (index=399).\n",
      "[2024-10-28 14:55:21] Processed 300 responses for LLM gpt_4o_mini_tuned_v1 (index=599).\n",
      "[2024-10-28 14:56:34] Processed 400 responses for LLM gpt_4o_mini_tuned_v1 (index=799).\n",
      "[2024-10-28 14:57:48] Processed 500 responses for LLM gpt_4o_mini_tuned_v1 (index=999).\n",
      "[2024-10-28 14:58:59] Processed 600 responses for LLM gpt_4o_mini_tuned_v1 (index=1199).\n",
      "[2024-10-28 15:00:17] Processed 700 responses for LLM gpt_4o_mini_tuned_v1 (index=1399).\n",
      "[2024-10-28 15:01:31] Processed 800 responses for LLM gpt_4o_mini_tuned_v1 (index=1599).\n",
      "[2024-10-28 15:02:52] Processed 900 responses for LLM gpt_4o_mini_tuned_v1 (index=1799).\n",
      "[2024-10-28 15:04:13] Processed 1000 responses for LLM gpt_4o_mini_tuned_v1 (index=1999).\n",
      "[2024-10-28 15:05:30] Processed 1100 responses for LLM gpt_4o_mini_tuned_v1 (index=2199).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1200c9e724c8febf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
