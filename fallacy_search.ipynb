{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fallacy Search Analysis",
   "id": "ad8039b3e62954ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:50:41.461760Z",
     "start_time": "2024-11-15T15:50:39.753703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.mafalda import get_mafalda_df, save_mafalda_df, evaluate_responses, get_mean_metrics\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ],
   "id": "bf5e9f6ad61348eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 3.1: Fallacy Search",
   "id": "5a81713385cee4ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:50:41.558365Z",
     "start_time": "2024-11-15T15:50:41.525064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename_e31 = 'data/mafalda_e31.csv'\n",
    "df_mafalda_e31 = get_mafalda_df(filename_e31)\n",
    "df_mafalda_e31.head()"
   ],
   "id": "62c8c9e9ae688408",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-15 16:50:41] Loaded existing mafalda dataframe from data/mafalda_e31.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  TITLE: Endless Ledge Skip Campaign for Alts PO...   \n",
       "1  Two of my best friends are really introverted,...   \n",
       "2  TITLE: There is a difference between a'smurf' ...   \n",
       "3  TITLE: Discussion Thread (Part 3): 2020 Presid...   \n",
       "4  America is the best place to live, because it'...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       [[155, 588, slippery slope]]   \n",
       "1                  [[84, 145, hasty generalization]]   \n",
       "2                        [[118, 265, false analogy]]   \n",
       "3  [[107, 261, guilt by association], [107, 338, ...   \n",
       "4                      [[0, 78, circular reasoning]]   \n",
       "\n",
       "                                            comments  \\\n",
       "0  ['Slippery slope: P1 = poster, A = why not jus...   \n",
       "1  [\"Based on two people only, you can't draw gen...   \n",
       "2  ['False Analogy: X: Having an alt , Y: smurfin...   \n",
       "3  ['Circular reasoning: X = The status quo in Am...   \n",
       "4  ['Circular reasoning: X=America is the best pl...   \n",
       "\n",
       "                               sentences_with_labels  \\\n",
       "0  {'TITLE: Endless Ledge Skip Campaign for Alts ...   \n",
       "1  {'Two of my best friends are really introverte...   \n",
       "2  {'TITLE: There is a difference between a'smurf...   \n",
       "3  {'TITLE: Discussion Thread (Part 3): 2020 Pres...   \n",
       "4  {'America is the best place to live, because i...   \n",
       "\n",
       "                                gpt_4o_mini_response  \n",
       "0  fallacies=[FallacyEntry(fallacy=<Fallacy.SLIPP...  \n",
       "1  fallacies=[FallacyEntry(fallacy=<Fallacy.HASTY...  \n",
       "2  fallacies=[FallacyEntry(fallacy=<Fallacy.FALSE...  \n",
       "3  fallacies=[FallacyEntry(fallacy=<Fallacy.HASTY...  \n",
       "4  fallacies=[FallacyEntry(fallacy=<Fallacy.CIRCU...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentences_with_labels</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TITLE: Endless Ledge Skip Campaign for Alts PO...</td>\n",
       "      <td>[[155, 588, slippery slope]]</td>\n",
       "      <td>['Slippery slope: P1 = poster, A = why not jus...</td>\n",
       "      <td>{'TITLE: Endless Ledge Skip Campaign for Alts ...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.SLIPP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two of my best friends are really introverted,...</td>\n",
       "      <td>[[84, 145, hasty generalization]]</td>\n",
       "      <td>[\"Based on two people only, you can't draw gen...</td>\n",
       "      <td>{'Two of my best friends are really introverte...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.HASTY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE: There is a difference between a'smurf' ...</td>\n",
       "      <td>[[118, 265, false analogy]]</td>\n",
       "      <td>['False Analogy: X: Having an alt , Y: smurfin...</td>\n",
       "      <td>{'TITLE: There is a difference between a'smurf...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.FALSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE: Discussion Thread (Part 3): 2020 Presid...</td>\n",
       "      <td>[[107, 261, guilt by association], [107, 338, ...</td>\n",
       "      <td>['Circular reasoning: X = The status quo in Am...</td>\n",
       "      <td>{'TITLE: Discussion Thread (Part 3): 2020 Pres...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.HASTY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is the best place to live, because it'...</td>\n",
       "      <td>[[0, 78, circular reasoning]]</td>\n",
       "      <td>['Circular reasoning: X=America is the best pl...</td>\n",
       "      <td>{'America is the best place to live, because i...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.CIRCU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scoring and Sanity Check",
   "id": "86fb92f5aa5743b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:02:39.178798Z",
     "start_time": "2024-11-15T16:02:38.362112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate_responses(df_mafalda_e31, confidence_threshold=0.5)\n",
    "\n",
    "save_mafalda_df(df_mafalda_e31, filename_e31)"
   ],
   "id": "6cfc4e8ae2ced447",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to match span for fallacy Fallacy.HASTY_GENERALIZATION:\n",
      "span: If you sell big amounts at 6k, he tried to bring the price even lower and cause a total sell off.\n",
      "text: TITLE: Now We Know Who Sold the Bottom at $6k And Tried To Crash Bitcoin - Spoiler alert: it was Mt.Gox trustee POST: can anyone explain the part where it says he tried to crash the market at 6k?.... i dont get the reasoning behind this? POST: If you sell at high prices, that makes sense, doesn't it? If you sell at bottom, you're either panicking or trying to crash the price of the asset you're selling. So if he sold big amounts at 6k, he tried to bring the price even lower and cause a total sell off. That is my logic.\n",
      "\n",
      "Warning: failed to match span for fallacy Fallacy.APPEAL_TO_AUTHORITY:\n",
      "span: Check [fightthenewdrug.org] for studies on the harmful effects of pornography.\n",
      "text: TITLE: Should I [23F] urge my husband [26M] to stop watching porn? POST: Yes. Porn has gained the support of popular culture, but it is unhealthy. It has been shown to make men more sexually aggressive, dissatisfied with their own sex lives, and contributes to the global sex trade. Viewing porn also causes prolonged dopamine exposure, which the brain will build resistance against - making it more difficult to feel joy from other daily activities. Porn is also highly addictive, and has diminishing returns over time, so a user will have to watch more porn, or more extreme porn, to get the same amount of dopamine. Check [fightthenewdrug.org](<URL>) for studies on the harmful effects of pornography. POST: Yeah, I always go to religious organizations to supply me with clear and objective scientific studies...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:02:41.629456Z",
     "start_time": "2024-11-15T16:02:41.613971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = get_mean_metrics(df_mafalda_e31)\n",
    "metrics.round(3)"
   ],
   "id": "4ce8d16348b4e9f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             precision  recall     f1\n",
       "gpt_4o_mini      0.467    0.61  0.403"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_4o_mini</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:17:53.524404Z",
     "start_time": "2024-11-15T16:17:42.981565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "confidence_ratings = []\n",
    "for fallacies in df_mafalda_e31['gpt_4o_mini_response']:\n",
    "    for _, entries in fallacies:\n",
    "        for entry in entries:\n",
    "            confidence_ratings.append(entry.confidence)\n",
    "\n",
    "confidence_ratings"
   ],
   "id": "fdce37f84df811eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.75,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.6,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 0.85,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.9,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8,\n",
       " 0.7,\n",
       " 0.8]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:56:41.162533Z",
     "start_time": "2024-11-15T15:53:05.467143Z"
    }
   },
   "cell_type": "code",
   "source": "test = 'test'",
   "id": "5b6df9dbdb28ee6c",
   "outputs": [],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
