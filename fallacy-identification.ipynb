{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T08:17:52.289739Z",
     "start_time": "2024-10-18T08:17:51.326845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from src.llms import get_llms, init_langchain, LLM\n",
    "from src.experiment import get_fallacy_df, save_fallacy_df, run_fallacy_identification_zero_shot\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "init_langchain()"
   ],
   "id": "801f0c9f57db9f3c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fallacy Identification Experiments",
   "id": "1d8205f73aece599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 1: Fallacy Identification with zero-shot Prompt",
   "id": "15c17be359103d5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T08:17:52.339582Z",
     "start_time": "2024-10-18T08:17:52.297691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "identification_zero_shot_filename = 'data/fallacy_identification_zero_shot.csv'\n",
    "df_fallacies = get_fallacy_df(identification_zero_shot_filename)\n",
    "df_fallacies.head()"
   ],
   "id": "86cb1cc7b84b2c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 10:17:52] Loaded existing fallacy identification dataframe from CSV.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category           type gpt_4o_response  \\\n",
       "0  Argument from Silence      1  informal  insufficiency             No.   \n",
       "1  Argument from Silence      1  informal  insufficiency             No.   \n",
       "2  Argument from Silence      1  informal  insufficiency             No.   \n",
       "3  Argument from Silence      1  informal  insufficiency             No.   \n",
       "4  Argument from Silence      1  informal  insufficiency             No.   \n",
       "\n",
       "  gpt_4_response gpt_4o_mini_response claude_3_5_sonnet_response  \\\n",
       "0             No                  No.                         No   \n",
       "1             No                  No.                         No   \n",
       "2             No                  No.                         No   \n",
       "3             No                  No.                         No   \n",
       "4             No                  No.                         No   \n",
       "\n",
       "  gemini_1_5_pro_response gemini_1_5_flash_response  gpt_4_score  \\\n",
       "0                      No                        No            1   \n",
       "1                      No                        No            1   \n",
       "2                      No                        No            1   \n",
       "3                     Yes                        No            1   \n",
       "4                      No                        No            1   \n",
       "\n",
       "   gpt_4o_score  gpt_4o_mini_score  claude_3_5_sonnet_score  \\\n",
       "0             1                  1                        1   \n",
       "1             1                  1                        1   \n",
       "2             1                  1                        1   \n",
       "3             1                  1                        1   \n",
       "4             1                  1                        1   \n",
       "\n",
       "   gemini_1_5_pro_score  gemini_1_5_flash_score  \n",
       "0                     1                       1  \n",
       "1                     1                       1  \n",
       "2                     1                       1  \n",
       "3                     0                       1  \n",
       "4                     1                       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gpt_4_score</th>\n",
       "      <th>gpt_4o_score</th>\n",
       "      <th>gpt_4o_mini_score</th>\n",
       "      <th>claude_3_5_sonnet_score</th>\n",
       "      <th>gemini_1_5_pro_score</th>\n",
       "      <th>gemini_1_5_flash_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T09:56:44.396680Z",
     "start_time": "2024-10-18T08:18:39.748448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.CLAUDE_3_OPUS, LLM.CLAUDE_3_HAIKU])\n",
    "# llms = get_llms([LLMLabels.GPT_4O])\n",
    "\n",
    "run_fallacy_identification_zero_shot(df_fallacies, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies, identification_zero_shot_filename)"
   ],
   "id": "b99e69538350883",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 10:20:13] Processed 100 responses for LLM claude_3_opus (index=99).\n",
      "[2024-10-18 10:21:42] Processed 200 responses for LLM claude_3_opus (index=199).\n",
      "[2024-10-18 10:23:47] Processed 300 responses for LLM claude_3_opus (index=299).\n",
      "[2024-10-18 10:26:08] Processed 400 responses for LLM claude_3_opus (index=399).\n",
      "[2024-10-18 10:27:42] Processed 500 responses for LLM claude_3_opus (index=499).\n",
      "[2024-10-18 10:29:45] Processed 600 responses for LLM claude_3_opus (index=599).\n",
      "[2024-10-18 10:32:13] Processed 700 responses for LLM claude_3_opus (index=699).\n",
      "[2024-10-18 10:33:43] Processed 800 responses for LLM claude_3_opus (index=799).\n",
      "[2024-10-18 10:35:57] Processed 900 responses for LLM claude_3_opus (index=899).\n",
      "[2024-10-18 10:38:22] Processed 1000 responses for LLM claude_3_opus (index=999).\n",
      "[2024-10-18 10:39:49] Processed 1100 responses for LLM claude_3_opus (index=1099).\n",
      "[2024-10-18 10:42:04] Processed 1200 responses for LLM claude_3_opus (index=1199).\n",
      "[2024-10-18 10:44:07] Processed 1300 responses for LLM claude_3_opus (index=1299).\n",
      "[2024-10-18 10:46:02] Processed 1400 responses for LLM claude_3_opus (index=1399).\n",
      "[2024-10-18 10:48:22] Processed 1500 responses for LLM claude_3_opus (index=1499).\n",
      "[2024-10-18 10:49:51] Processed 1600 responses for LLM claude_3_opus (index=1599).\n",
      "[2024-10-18 10:51:55] Processed 1700 responses for LLM claude_3_opus (index=1699).\n",
      "[2024-10-18 10:54:29] Processed 1800 responses for LLM claude_3_opus (index=1799).\n",
      "[2024-10-18 10:55:59] Processed 1900 responses for LLM claude_3_opus (index=1899).\n",
      "[2024-10-18 10:58:28] Processed 2000 responses for LLM claude_3_opus (index=1999).\n",
      "[2024-10-18 10:59:58] Processed 2100 responses for LLM claude_3_opus (index=2099).\n",
      "[2024-10-18 11:04:40] Processed 2200 responses for LLM claude_3_opus (index=2199).\n",
      "[2024-10-18 11:06:16] Processed 2300 responses for LLM claude_3_opus (index=2299).\n",
      "[2024-10-18 11:07:58] Processed 2400 responses for LLM claude_3_opus (index=2399).\n",
      "[2024-10-18 11:09:56] Processed 2500 responses for LLM claude_3_opus (index=2499).\n",
      "[2024-10-18 11:12:02] Processed 2600 responses for LLM claude_3_opus (index=2599).\n",
      "[2024-10-18 11:14:00] Processed 2700 responses for LLM claude_3_opus (index=2699).\n",
      "[2024-10-18 11:15:48] Processed 2800 responses for LLM claude_3_opus (index=2799).\n",
      "[2024-10-18 11:18:03] Processed 2900 responses for LLM claude_3_opus (index=2899).\n",
      "[2024-10-18 11:20:38] Processed 3000 responses for LLM claude_3_opus (index=2999).\n",
      "[2024-10-18 11:22:08] Processed 3100 responses for LLM claude_3_opus (index=3099).\n",
      "[2024-10-18 11:24:28] Processed 3200 responses for LLM claude_3_opus (index=3199).\n",
      "[2024-10-18 11:25:59] Processed 3300 responses for LLM claude_3_opus (index=3299).\n",
      "[2024-10-18 11:28:04] Processed 3400 responses for LLM claude_3_opus (index=3399).\n",
      "[2024-10-18 11:30:25] Processed 3500 responses for LLM claude_3_opus (index=3499).\n",
      "[2024-10-18 11:31:56] Processed 3600 responses for LLM claude_3_opus (index=3599).\n",
      "[2024-10-18 11:34:05] Processed 3700 responses for LLM claude_3_opus (index=3699).\n",
      "[2024-10-18 11:36:26] Processed 3800 responses for LLM claude_3_opus (index=3799).\n",
      "[2024-10-18 11:37:55] Processed 3900 responses for LLM claude_3_opus (index=3899).\n",
      "[2024-10-18 11:40:05] Processed 4000 responses for LLM claude_3_opus (index=3999).\n",
      "[2024-10-18 11:42:34] Processed 4100 responses for LLM claude_3_opus (index=4099).\n",
      "[2024-10-18 11:44:05] Processed 4200 responses for LLM claude_3_opus (index=4199).\n",
      "[2024-10-18 11:46:21] Processed 4300 responses for LLM claude_3_opus (index=4299).\n",
      "[2024-10-18 11:47:51] Processed 4400 responses for LLM claude_3_opus (index=4399).\n",
      "[2024-10-18 11:50:21] Processed 4500 responses for LLM claude_3_opus (index=4499).\n",
      "[2024-10-18 11:51:52] Processed 4600 responses for LLM claude_3_opus (index=4599).\n",
      "[2024-10-18 11:55:04] Processed 100 responses for LLM claude_3_haiku (index=99).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_base_client.py:1037\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1037\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m err:  \u001B[38;5;66;03m# thrown on 4xx and 5xx status code\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[0;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Client error '429 Too Many Requests' for url 'https://api.anthropic.com/v1/messages'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m llms \u001B[38;5;241m=\u001B[39m get_llms([LLM\u001B[38;5;241m.\u001B[39mCLAUDE_3_OPUS, LLM\u001B[38;5;241m.\u001B[39mCLAUDE_3_HAIKU])\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# llms = get_llms([LLMLabels.GPT_4O])\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m run_fallacy_identification(df_fallacies, llms, sleep_seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      6\u001B[0m save_fallacy_identification_df(df_fallacies)\n",
      "File \u001B[0;32m~/HSLU/Thesis/fallacy-detection/src/experiment.py:48\u001B[0m, in \u001B[0;36mrun_fallacy_identification\u001B[0;34m(df_fallacies, llms, keep_existing_responses, sleep_seconds)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# log(f\"Prompting LLM {llm_name.value}: {prompt}\")\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     response \u001B[38;5;241m=\u001B[39m llm\u001B[38;5;241m.\u001B[39minvoke(prompt)\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;66;03m# log(f\"Response from LLM {llm_name.value}: {response}\")\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \n\u001B[1;32m     51\u001B[0m     \u001B[38;5;66;03m# Truncate the response to 10 characters in case instructions are ignored\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     response_text \u001B[38;5;241m=\u001B[39m response \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    285\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 286\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    287\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    288\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    289\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    290\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    291\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    292\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    293\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    294\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    295\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    647\u001B[0m ]\n\u001B[1;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 633\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    634\u001B[0m                 m,\n\u001B[1;32m    635\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    636\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    637\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    638\u001B[0m             )\n\u001B[1;32m    639\u001B[0m         )\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    852\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    853\u001B[0m         )\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:789\u001B[0m, in \u001B[0;36mChatAnthropic._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    787\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m generate_from_stream(stream_iter)\n\u001B[1;32m    788\u001B[0m payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_request_payload(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 789\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mmessages\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[1;32m    790\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_output(data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_utils/_utils.py:274\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    272\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/resources/messages.py:885\u001B[0m, in \u001B[0;36mMessages.create\u001B[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m DEPRECATED_MODELS:\n\u001B[1;32m    879\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    880\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated and will reach end-of-life on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDEPRECATED_MODELS[model]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m    882\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m    883\u001B[0m     )\n\u001B[0;32m--> 885\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    886\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/v1/messages\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    887\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    888\u001B[0m         {\n\u001B[1;32m    889\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m    890\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m    891\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    892\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    893\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop_sequences,\n\u001B[1;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m    895\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m: system,\n\u001B[1;32m    896\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    897\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m    898\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    899\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_k\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_k,\n\u001B[1;32m    900\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    901\u001B[0m         },\n\u001B[1;32m    902\u001B[0m         message_create_params\u001B[38;5;241m.\u001B[39mMessageCreateParams,\n\u001B[1;32m    903\u001B[0m     ),\n\u001B[1;32m    904\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    905\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    906\u001B[0m     ),\n\u001B[1;32m    907\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mMessage,\n\u001B[1;32m    908\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    909\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mStream[RawMessageStreamEvent],\n\u001B[1;32m    910\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_base_client.py:1277\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1263\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1264\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1265\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1272\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1273\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1274\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1275\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1276\u001B[0m     )\n\u001B[0;32m-> 1277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_base_client.py:954\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    952\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 954\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    955\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    956\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    957\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    958\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    959\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m    960\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_base_client.py:1043\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[1;32m   1042\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1043\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[1;32m   1044\u001B[0m         input_options,\n\u001B[1;32m   1045\u001B[0m         cast_to,\n\u001B[1;32m   1046\u001B[0m         retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m   1047\u001B[0m         response_headers\u001B[38;5;241m=\u001B[39merr\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m   1048\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1049\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1050\u001B[0m     )\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/anthropic/_base_client.py:1090\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1086\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying request to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m, options\u001B[38;5;241m.\u001B[39murl, timeout)\n\u001B[1;32m   1088\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1089\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m-> 1090\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m   1092\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1093\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1094\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1097\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1098\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_fallacies.head()",
   "id": "d971e94c8e05d198",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
