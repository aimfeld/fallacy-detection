{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from src.llms import get_llms, init_langchain, LLM\n",
    "from src.experiment import get_fallacy_df, save_fallacy_df, run_experiment\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "init_langchain()"
   ],
   "id": "801f0c9f57db9f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fallacy Identification Experiments",
   "id": "1d8205f73aece599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 1: Fallacy Identification with zero-shot Prompt",
   "id": "15c17be359103d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "e1_filename = 'data/fallacies_e1.csv'\n",
    "df_fallacies_e1 = get_fallacy_df(e1_filename)\n",
    "df_fallacies_e1.head()"
   ],
   "id": "86cb1cc7b84b2c91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.CLAUDE_3_HAIKU])\n",
    "\n",
    "prompt_template_e1 = \"Is the following reasoning step correct? You can only answer \\\"Yes\\\" or \\\"No\\\".\\n[step]\"\n",
    "run_experiment(df_fallacies_e1, e1_filename, prompt_template_e1, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e1, e1_filename)"
   ],
   "id": "b99e69538350883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 2: Fallacy Identification with few-shot Prompt",
   "id": "60615736cb645e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "e2_filename = 'data/fallacies_e2.csv'\n",
    "df_fallacies_e2 = get_fallacy_df(e2_filename)\n",
    "df_fallacies_e2.head()"
   ],
   "id": "d971e94c8e05d198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T12:56:18.947886Z",
     "start_time": "2024-10-18T12:56:18.831626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.GPT_4O, LLM.CLAUDE_3_5_SONNET, LLM.GEMINI_1_5_PRO])\n",
    "\n",
    "prompt_template_e2 = \"\"\"Is the following reasoning step correct? You can only answer \\\"Yes\\\" or \\\"No\\\".\n",
    "Since if it's raining then the streets are wet and it's raining now, therefore, the streets are wet.\n",
    "Yes.\n",
    "Since I found a shell on the beach and this shell was beautifully shaped and colored, therefore, all shells are beautifully shaped and colored.\n",
    "No.\n",
    "Since I am at home or I am in the city and I am at home, therefore, I am not in the city.\n",
    "No.\n",
    "Since heavy snowfall often leads to traffic jams and traffic jams cause delays, therefore, heavy snowfall can lead to delays.\n",
    "Yes.\n",
    "[step]\"\"\"\n",
    "\n",
    "run_experiment(df_fallacies_e2, e2_filename, prompt_template_e2, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e2, e2_filename)\n"
   ],
   "id": "b24032214d1174f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\\nSince if it\\'s raining then the streets are wet and it\\'s raining now, therefore, the streets are wet.\\nYes.\\nSince I found a shell on the beach and this shell was beautifully shaped and colored, therefore, all shells are beautifully shaped and colored.\\nNo.\\nSince I am at home or I am in the city and I am at home, therefore, I am not in the city.\\nNo.\\nSince heavy snowfall often leads to traffic jams and traffic jams cause delays, therefore, heavy snowfall can lead to delays.\\nYes.\\nSome question.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "78a259e1cc2663ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
