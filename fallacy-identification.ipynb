{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:06:39.736089Z",
     "start_time": "2024-10-18T16:06:38.771068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from src.llms import get_llms, init_langchain, LLM\n",
    "from src.experiment import get_fallacy_df, save_fallacy_df, run_experiment\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "init_langchain()"
   ],
   "id": "801f0c9f57db9f3c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fallacy Identification Experiments",
   "id": "1d8205f73aece599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 1: Fallacy Identification with zero-shot Prompt",
   "id": "15c17be359103d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "e1_filename = 'data/fallacies_e1.csv'\n",
    "df_fallacies_e1 = get_fallacy_df(e1_filename)\n",
    "df_fallacies_e1.head()"
   ],
   "id": "86cb1cc7b84b2c91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llms = get_llms([LLM.CLAUDE_3_HAIKU])\n",
    "\n",
    "prompt_template_e1 = \"\"\"Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
    "[step]\"\"\"\n",
    "run_experiment(df_fallacies_e1, e1_filename, prompt_template_e1, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e1, e1_filename)"
   ],
   "id": "b99e69538350883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 2: Fallacy Identification with few-shot Prompt",
   "id": "60615736cb645e61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:06:45.250459Z",
     "start_time": "2024-10-18T16:06:45.081401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "e2_filename = 'data/fallacies_e2.csv'\n",
    "df_fallacies_e2 = get_fallacy_df(e2_filename)\n",
    "df_fallacies_e2.head()"
   ],
   "id": "d971e94c8e05d198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 18:06:45] Loaded existing fallacy dataframe from data/fallacies_e2.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category           type gpt_4o_response  \\\n",
       "0  Argument from Silence      1  informal  insufficiency             No.   \n",
       "1  Argument from Silence      1  informal  insufficiency             No.   \n",
       "2  Argument from Silence      1  informal  insufficiency             No.   \n",
       "3  Argument from Silence      1  informal  insufficiency             No.   \n",
       "4  Argument from Silence      1  informal  insufficiency             No.   \n",
       "\n",
       "  claude_3_5_sonnet_response gemini_1_5_pro_response  \n",
       "0                        No.                     No.  \n",
       "1                        No.                     No.  \n",
       "2                        No.                     No.  \n",
       "3                        No.                    Yes.  \n",
       "4                        No.                     No.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T17:38:14.604975Z",
     "start_time": "2024-10-18T16:06:52.521166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# llms = get_llms([LLM.GPT_4O, LLM.CLAUDE_3_5_SONNET, LLM.GEMINI_1_5_PRO])\n",
    "llms = get_llms([LLM.GEMINI_1_5_PRO])\n",
    "\n",
    "prompt_template_e2 = \"\"\"Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
    "Since if it's raining then the streets are wet and it's raining now, therefore, the streets are wet.\n",
    "Yes.\n",
    "Since I found a shell on the beach and this shell was beautifully shaped and colored, therefore, all shells are beautifully shaped and colored.\n",
    "No.\n",
    "Since I am at home or I am in the city and I am at home, therefore, I am not in the city.\n",
    "No.\n",
    "Since heavy snowfall often leads to traffic jams and traffic jams cause delays, therefore, heavy snowfall can lead to delays.\n",
    "Yes.\n",
    "[step]\"\"\"\n",
    "\n",
    "run_experiment(df_fallacies_e2, e2_filename, prompt_template_e2, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e2, e2_filename)\n"
   ],
   "id": "b24032214d1174f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 18:09:36] Processed 100 responses for LLM gemini_1_5_pro (index=1379).\n",
      "[2024-10-18 18:12:07] Processed 200 responses for LLM gemini_1_5_pro (index=1479).\n",
      "[2024-10-18 18:13:16] Processed 300 responses for LLM gemini_1_5_pro (index=1579).\n",
      "[2024-10-18 18:16:18] Processed 400 responses for LLM gemini_1_5_pro (index=1679).\n",
      "[2024-10-18 18:20:58] Processed 500 responses for LLM gemini_1_5_pro (index=1779).\n",
      "[2024-10-18 18:23:11] Processed 600 responses for LLM gemini_1_5_pro (index=1879).\n",
      "[2024-10-18 18:25:52] Processed 700 responses for LLM gemini_1_5_pro (index=1979).\n",
      "[2024-10-18 18:27:48] Processed 800 responses for LLM gemini_1_5_pro (index=2079).\n",
      "[2024-10-18 18:30:36] Processed 900 responses for LLM gemini_1_5_pro (index=2179).\n",
      "[2024-10-18 18:36:03] Processed 1000 responses for LLM gemini_1_5_pro (index=2279).\n",
      "[2024-10-18 18:38:13] Processed 1100 responses for LLM gemini_1_5_pro (index=2379).\n",
      "[2024-10-18 18:42:46] Processed 1200 responses for LLM gemini_1_5_pro (index=2479).\n",
      "[2024-10-18 18:45:21] Processed 1300 responses for LLM gemini_1_5_pro (index=2579).\n",
      "[2024-10-18 18:46:29] Processed 1400 responses for LLM gemini_1_5_pro (index=2679).\n",
      "[2024-10-18 18:49:05] Processed 1500 responses for LLM gemini_1_5_pro (index=2779).\n",
      "[2024-10-18 18:52:41] Processed 1600 responses for LLM gemini_1_5_pro (index=2879).\n",
      "[2024-10-18 18:55:29] Processed 1700 responses for LLM gemini_1_5_pro (index=2979).\n",
      "[2024-10-18 18:57:28] Processed 1800 responses for LLM gemini_1_5_pro (index=3079).\n",
      "[2024-10-18 18:59:53] Processed 1900 responses for LLM gemini_1_5_pro (index=3179).\n",
      "[2024-10-18 19:02:23] Processed 2000 responses for LLM gemini_1_5_pro (index=3279).\n",
      "[2024-10-18 19:05:09] Processed 2100 responses for LLM gemini_1_5_pro (index=3379).\n",
      "[2024-10-18 19:08:49] Processed 2200 responses for LLM gemini_1_5_pro (index=3479).\n",
      "[2024-10-18 19:10:50] Processed 2300 responses for LLM gemini_1_5_pro (index=3579).\n",
      "[2024-10-18 19:14:25] Processed 2400 responses for LLM gemini_1_5_pro (index=3679).\n",
      "[2024-10-18 19:19:20] Processed 2500 responses for LLM gemini_1_5_pro (index=3779).\n",
      "[2024-10-18 19:22:52] Processed 2600 responses for LLM gemini_1_5_pro (index=3879).\n",
      "[2024-10-18 19:24:02] Processed 2700 responses for LLM gemini_1_5_pro (index=3979).\n",
      "[2024-10-18 19:25:12] Processed 2800 responses for LLM gemini_1_5_pro (index=4079).\n",
      "[2024-10-18 19:28:36] Processed 2900 responses for LLM gemini_1_5_pro (index=4179).\n",
      "[2024-10-18 19:29:45] Processed 3000 responses for LLM gemini_1_5_pro (index=4279).\n",
      "[2024-10-18 19:30:56] Processed 3100 responses for LLM gemini_1_5_pro (index=4379).\n",
      "[2024-10-18 19:34:48] Processed 3200 responses for LLM gemini_1_5_pro (index=4479).\n",
      "[2024-10-18 19:37:32] Processed 3300 responses for LLM gemini_1_5_pro (index=4579).\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 3: Fallacy Identification with chain-of-thought Prompt",
   "id": "b7f0494cfd3f1840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:04:06.944615876Z",
     "start_time": "2024-10-18T13:02:10.766440Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 15:02:10] Created new fallacy identification dataframe.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category           type  \n",
       "0  Argument from Silence      1  informal  insufficiency  \n",
       "1  Argument from Silence      1  informal  insufficiency  \n",
       "2  Argument from Silence      1  informal  insufficiency  \n",
       "3  Argument from Silence      1  informal  insufficiency  \n",
       "4  Argument from Silence      1  informal  insufficiency  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "e3_filename = 'data/fallacies_e3.csv'\n",
    "df_fallacies_e3 = get_fallacy_df(e3_filename)\n",
    "df_fallacies_e3.head()"
   ],
   "id": "381c9adbd379caa4"
  },
  {
   "metadata": {
    "jupyter": {},
    "ExecuteTime": {
     "end_time": "2024-10-18T16:04:06.945319660Z",
     "start_time": "2024-10-18T13:03:04.532345Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 15:04:00] Processed 100 responses for LLM gpt_4o (index=99).\n",
      "[2024-10-18 15:04:45] Processed 200 responses for LLM gpt_4o (index=199).\n",
      "[2024-10-18 15:05:32] Processed 300 responses for LLM gpt_4o (index=299).\n",
      "[2024-10-18 15:06:21] Processed 400 responses for LLM gpt_4o (index=399).\n",
      "[2024-10-18 15:07:10] Processed 500 responses for LLM gpt_4o (index=499).\n",
      "[2024-10-18 15:08:15] Processed 600 responses for LLM gpt_4o (index=599).\n",
      "[2024-10-18 15:09:02] Processed 700 responses for LLM gpt_4o (index=699).\n",
      "[2024-10-18 15:09:49] Processed 800 responses for LLM gpt_4o (index=799).\n",
      "[2024-10-18 15:10:38] Processed 900 responses for LLM gpt_4o (index=899).\n",
      "[2024-10-18 15:11:24] Processed 1000 responses for LLM gpt_4o (index=999).\n",
      "[2024-10-18 15:12:09] Processed 1100 responses for LLM gpt_4o (index=1099).\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "llms = get_llms([LLM.GPT_4O, LLM.CLAUDE_3_5_SONNET, LLM.GEMINI_1_5_PRO])\n",
    "\n",
    "prompt_template_e3 = \"\"\"Is the following reasoning step correct?\n",
    "Let's think step by step and then answer \"Yes\" or \"No\".\n",
    "[step]\"\"\"\n",
    "\n",
    "run_experiment(df_fallacies_e3, e3_filename, prompt_template_e3, llms, sleep_seconds=0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e3, e3_filename)\n"
   ],
   "id": "a0c8bd26ad1fafd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc01dd3d2e7c6e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
