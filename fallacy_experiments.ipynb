{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1955b8d3b049a282",
   "metadata": {},
   "source": [
    "# Fallacy Experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!--TABLE OF CONTENTS-->\n",
    "**Contents**\n",
    "\n",
    "- [Fallacy Identification](#Fallacy-Identification)\n",
    "  - [Experiment 1.1: Fallacy Identification with Zero-Shot Prompt](#Experiment-1.1:-Fallacy-Identification-with-Zero-Shot-Prompt)\n",
    "  - [Experiment 1.2: Fallacy Identification with Few-Shot Prompt](#Experiment-1.2:-Fallacy-Identification-with-Few-Shot-Prompt)\n",
    "  - [Experiment 1.3: Fallacy Identification with Chain-of-Thought Prompt](#Experiment-1.3:-Fallacy-Identification-with-Chain-of-Thought-Prompt)\n",
    "  - [Experiment 1.4: Fallacy Identification with Fine-Tuning](#Experiment-1.4:-Fallacy-Identification-with-Fine-Tuning)\n",
    "  - [Experiment 1.5: Fallacy Identification with Rephrased Prompt](#Experiment-1.5:-Fallacy-Identification-with-Rephrased-Prompt)\n",
    "  - [Experiment 1.6: Fallacy Identification Reliability Test](#Experiment-1.6:-Fallacy-Identification-Reliability-Test)\n",
    "- [Fallacy Classification](#Fallacy-Classification)\n",
    "  - [Experiment 2.1: Fallacy Classification with Zero-Shot Prompt](#Experiment-2.1:-Fallacy-Classification-with-Zero-Shot-Prompt)\n",
    "  - [Experiment 2.2: Fallacy Classification with Fine-Tuning](#Experiment-2.2:-Fallacy-Classification-with-Fine-Tuning)\n",
    "- [Fallacy Search](#Fallacy-Search)\n",
    "  - [Experiment 3.1: Fallacy Search](#Experiment-3.1:-Fallacy-Search)"
   ],
   "id": "1779cd1ad68dce2a"
  },
  {
   "cell_type": "code",
   "id": "801f0c9f57db9f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:08:53.675970Z",
     "start_time": "2025-02-04T09:08:52.510318Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "from src.llms import get_llms, get_fallacy_search_llms, init_langchain, LLM\n",
    "from src.tuning import TuningSet\n",
    "from src.fallacies import get_fallacy_df, save_fallacy_df\n",
    "from src.mafalda import get_mafalda_df, save_mafalda_df\n",
    "from src.experiment import (\n",
    "    run_experiment,\n",
    "    get_identification_zero_shot_prompt_template,\n",
    "    get_identification_zero_shot_prompt_template_v2,\n",
    "    get_identification_few_shot_prompt_template,\n",
    "    get_identification_cot_prompt_template,\n",
    "    get_classification_prompt_template,\n",
    "    get_mafalda_search_system_prompt_v2\n",
    ")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Streaming with Pydantic response_format not yet supported.\")\n",
    "\n",
    "init_langchain()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "2f70cdc3be871171",
   "metadata": {},
   "source": [
    "## Fallacy Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c17be359103d5e",
   "metadata": {},
   "source": [
    "### Experiment 1.1: Fallacy Identification with Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "86cb1cc7b84b2c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:08:54.873718Z",
     "start_time": "2025-02-04T09:08:54.233225Z"
    }
   },
   "source": [
    "filename_e11 = 'data/fallacies_e11.csv'\n",
    "df_fallacies_e11 = get_fallacy_df(filename_e11)\n",
    "df_fallacies_e11.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 10:08:54] Loaded existing fallacy dataframe from data/fallacies_e11.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy label  category    subcategory gpt_4o_response  \\\n",
       "0  Argument from Silence     1  informal  insufficiency             No.   \n",
       "1  Argument from Silence     1  informal  insufficiency             No.   \n",
       "2  Argument from Silence     1  informal  insufficiency             No.   \n",
       "3  Argument from Silence     1  informal  insufficiency             No.   \n",
       "4  Argument from Silence     1  informal  insufficiency             No.   \n",
       "\n",
       "  gpt_4_response gpt_4o_mini_response claude_3_5_sonnet_response  ...  \\\n",
       "0             No                  No.                         No  ...   \n",
       "1             No                  No.                         No  ...   \n",
       "2             No                  No.                         No  ...   \n",
       "3             No                  No.                         No  ...   \n",
       "4             No                  No.                         No  ...   \n",
       "\n",
       "  mistral_small_2_pred mistral_small_2_score o1_mini_pred o1_mini_score  \\\n",
       "0                    1                     1          1.0             1   \n",
       "1                    1                     1          1.0             1   \n",
       "2                    1                     1          1.0             1   \n",
       "3                    1                     1          1.0             1   \n",
       "4                    1                     1          1.0             1   \n",
       "\n",
       "                            deepseek_r1_14b_response  \\\n",
       "0  <think> Okay, so I'm trying to figure out whet...   \n",
       "1  <think> Okay, so I'm trying to figure out whet...   \n",
       "2  <think> Okay, so I'm trying to figure out if t...   \n",
       "3  <think> Okay, so I'm trying to figure out if t...   \n",
       "4  <think> Okay, so I'm trying to figure out if t...   \n",
       "\n",
       "                           deepseek_r1_671b_response deepseek_r1_14b_pred  \\\n",
       "0  <think>Okay, let's see. The user is asking if ...                  1.0   \n",
       "1  <think> Okay, let's see. The question is wheth...                  1.0   \n",
       "2  <think> Okay, so I need to determine if the re...                  1.0   \n",
       "3  <think> Okay, let's tackle this question. So, ...                  1.0   \n",
       "4  <think> Okay, let me think about this. So the ...                  1.0   \n",
       "\n",
       "  deepseek_r1_14b_score deepseek_r1_671b_pred deepseek_r1_671b_score  \n",
       "0                     1                   1.0                      1  \n",
       "1                     1                   1.0                      1  \n",
       "2                     1                   1.0                      1  \n",
       "3                     1                   1.0                      1  \n",
       "4                     1                   1.0                      1  \n",
       "\n",
       "[5 rows x 54 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>...</th>\n",
       "      <th>mistral_small_2_pred</th>\n",
       "      <th>mistral_small_2_score</th>\n",
       "      <th>o1_mini_pred</th>\n",
       "      <th>o1_mini_score</th>\n",
       "      <th>deepseek_r1_14b_response</th>\n",
       "      <th>deepseek_r1_671b_response</th>\n",
       "      <th>deepseek_r1_14b_pred</th>\n",
       "      <th>deepseek_r1_14b_score</th>\n",
       "      <th>deepseek_r1_671b_pred</th>\n",
       "      <th>deepseek_r1_671b_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;think&gt; Okay, so I'm trying to figure out whet...</td>\n",
       "      <td>&lt;think&gt;Okay, let's see. The user is asking if ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;think&gt; Okay, so I'm trying to figure out whet...</td>\n",
       "      <td>&lt;think&gt; Okay, let's see. The question is wheth...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;think&gt; Okay, so I'm trying to figure out if t...</td>\n",
       "      <td>&lt;think&gt; Okay, so I need to determine if the re...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;think&gt; Okay, so I'm trying to figure out if t...</td>\n",
       "      <td>&lt;think&gt; Okay, let's tackle this question. So, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;think&gt; Okay, so I'm trying to figure out if t...</td>\n",
       "      <td>&lt;think&gt; Okay, let me think about this. So the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1eddb3ddf6410c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:08:56.455340Z",
     "start_time": "2025-02-04T09:08:56.430770Z"
    }
   },
   "source": [
    "prompt_template_e11 = get_identification_zero_shot_prompt_template()\n",
    "print(prompt_template_e11)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
      "[text]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b99e69538350883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:10:16.328303Z",
     "start_time": "2025-02-04T09:09:17.737622Z"
    }
   },
   "source": [
    "llms = get_llms([LLM.DEEPSEEK_R1_671B])\n",
    "\n",
    "run_experiment(df_fallacies_e11, filename_e11, prompt_template_e11, llms, sleep_seconds=0.0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e11, filename_e11)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6722dd565f693c1d",
   "metadata": {},
   "source": [
    "OpenAI o1-preview cost:\n",
    "- 60 zero-shot prompt responses was \\$1.64\n",
    "- Estimated cost for 4640 prompts is \\$126.83\n",
    "\n",
    "OpenAI o1-mini cost:\n",
    "- 100 zero-shot prompt responses was \\$0.38\n",
    "- Estimated cost for 4640 prompts is \\$17.63\n",
    "- Actual cost for 4640 prompts was \\$15.89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60615736cb645e61",
   "metadata": {},
   "source": [
    "### Experiment 1.2: Fallacy Identification with Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d971e94c8e05d198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:14:12.206161Z",
     "start_time": "2024-10-31T13:14:12.157969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 14:14:12] Loaded existing fallacy dataframe from data/fallacies_e12.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "      <th>llama_3_1_8b_response</th>\n",
       "      <th>mistral_large_2_response</th>\n",
       "      <th>mistral_small_2_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No.</td>\n",
       "      <td>Yes. No. No. Yes. No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory gpt_4o_response  \\\n",
       "0  Argument from Silence      1  informal  insufficiency             No.   \n",
       "1  Argument from Silence      1  informal  insufficiency             No.   \n",
       "2  Argument from Silence      1  informal  insufficiency             No.   \n",
       "3  Argument from Silence      1  informal  insufficiency             No.   \n",
       "4  Argument from Silence      1  informal  insufficiency             No.   \n",
       "\n",
       "  claude_3_5_sonnet_response gemini_1_5_pro_response gpt_4o_mini_response  \\\n",
       "0                        No.                     No.                  No.   \n",
       "1                        No.                     No.                  No.   \n",
       "2                        No.                     No.                  No.   \n",
       "3                        No.                    Yes.                  No.   \n",
       "4                        No.                     No.                  No.   \n",
       "\n",
       "  claude_3_haiku_response gemini_1_5_flash_response  \\\n",
       "0                     No.                        No   \n",
       "1                     No.                        No   \n",
       "2                     No.                        No   \n",
       "3                     No.                        No   \n",
       "4                     No.                        No   \n",
       "\n",
       "  gemini_1_5_flash_8b_response llama_3_1_70b_response llama_3_1_8b_response  \\\n",
       "0                           No                    No.                   No.   \n",
       "1                           No                    No.                   No.   \n",
       "2                           No                    No.                   No.   \n",
       "3                           No                    Yes                   No.   \n",
       "4                           No                    No.                   No.   \n",
       "\n",
       "  mistral_large_2_response mistral_small_2_response  \n",
       "0                      No.                      No.  \n",
       "1                      No.                      No.  \n",
       "2                      No.                      No.  \n",
       "3    Yes. No. No. Yes. No.                      No.  \n",
       "4                      No.                      No.  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_12 = 'data/fallacies_e12.csv'\n",
    "df_fallacies_e12 = get_fallacy_df(filename_12)\n",
    "df_fallacies_e12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4efeaa22613e3fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:14:13.014238Z",
     "start_time": "2024-10-31T13:14:12.987817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
      "Since if it's raining then the streets are wet and it's raining now, therefore, the streets are wet.\n",
      "Yes.\n",
      "Since I found a shell on the beach and this shell was beautifully shaped and colored, therefore, all shells are beautifully shaped and colored.\n",
      "No.\n",
      "Since I am at home or I am in the city and I am at home, therefore, I am not in the city.\n",
      "No.\n",
      "Since heavy snowfall often leads to traffic jams and traffic jams cause delays, therefore, heavy snowfall can lead to delays.\n",
      "Yes.\n",
      "[step]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_e12 = get_identification_few_shot_prompt_template()\n",
    "print(prompt_template_e12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b24032214d1174f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:14:15.550228Z",
     "start_time": "2024-10-31T13:14:13.899157Z"
    }
   },
   "outputs": [],
   "source": [
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "run_experiment(df_fallacies_e12, filename_12, prompt_template_e12, llms, sleep_seconds=0.2)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e12, filename_12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0494cfd3f1840",
   "metadata": {},
   "source": [
    "### Experiment 1.3: Fallacy Identification with Chain-of-Thought Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381c9adbd379caa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T07:22:52.836923Z",
     "start_time": "2024-11-01T07:22:52.446864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 08:22:52] Loaded existing fallacy dataframe from data/fallacies_e13.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "      <th>llama_3_1_8b_response</th>\n",
       "      <th>mistral_large_2_response</th>\n",
       "      <th>mistral_small_2_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not necessarily corr...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Joh...</td>\n",
       "      <td>No.  Silence is not necessarily an admission o...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. John aske...</td>\n",
       "      <td>No.  Silence doesn't necessarily mean guilt or...</td>\n",
       "      <td>No.  Silence doesn't necessarily mean agreemen...</td>\n",
       "      <td>To evaluate the correctness of the reasoning s...</td>\n",
       "      <td>No.  This reasoning step is incorrect because ...</td>\n",
       "      <td>No, the reasoning step is not necessarily corr...</td>\n",
       "      <td>No, the reasoning step is not correct. Maria's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. Bob's s...</td>\n",
       "      <td>Let's think through this step by step:  1. Ali...</td>\n",
       "      <td>No.  Silence could indicate a lack of knowledg...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Alice ask...</td>\n",
       "      <td>No.  Silence doesn't imply knowledge.  Bob's s...</td>\n",
       "      <td>No.  Silence does not equate to knowledge.  Bo...</td>\n",
       "      <td>No, the reasoning step is not correct.   The c...</td>\n",
       "      <td>No.  This reasoning is incorrect because remai...</td>\n",
       "      <td>No. Bob's silence does not necessarily indicat...</td>\n",
       "      <td>No, the reasoning step is not correct. Bob's s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. The con...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Ali...</td>\n",
       "      <td>No.  Bob's silence doesn't confirm Alice's cla...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Alice cla...</td>\n",
       "      <td>No.  Bob's silence doesn't provide evidence su...</td>\n",
       "      <td>No.  Alice's claim, even if true, and Bob's si...</td>\n",
       "      <td>To evaluate the correctness of the reasoning s...</td>\n",
       "      <td>No.  This reasoning is incorrect because it as...</td>\n",
       "      <td>No, the reasoning step is not correct. Bob's s...</td>\n",
       "      <td>No, the reasoning step is not correct. Just be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not necessarily corr...</td>\n",
       "      <td>Let's think through this step-by-step:  1. The...</td>\n",
       "      <td>No.  While Tom's experience is valuable, the a...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. Tom, a se...</td>\n",
       "      <td>No.  The reasoning commits an appeal to author...</td>\n",
       "      <td>No.  Just because one person says something an...</td>\n",
       "      <td>To evaluate the correctness of the reasoning s...</td>\n",
       "      <td>No.  The reasoning step is incorrect for sever...</td>\n",
       "      <td>No, the reasoning step is not necessarily corr...</td>\n",
       "      <td>No, the reasoning step is not correct. Here's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No. The reasoning step is not correct. The fac...</td>\n",
       "      <td>Let's think through this step-by-step:  1. Joh...</td>\n",
       "      <td>No.  Silence does not imply guilt or fear.  Ma...</td>\n",
       "      <td>No.</td>\n",
       "      <td>Let's analyze this step-by-step:  1. John accu...</td>\n",
       "      <td>No.  Silence does not equal guilt or fear.  Ma...</td>\n",
       "      <td>No.  Silence in the face of an accusation does...</td>\n",
       "      <td>No, the reasoning step is not correct.   Here'...</td>\n",
       "      <td>No.  This reasoning step is incorrect because ...</td>\n",
       "      <td>No. Mary's silence does not necessarily mean s...</td>\n",
       "      <td>No, the reasoning step is not correct. Just be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory  \\\n",
       "0  Argument from Silence      1  informal  insufficiency   \n",
       "1  Argument from Silence      1  informal  insufficiency   \n",
       "2  Argument from Silence      1  informal  insufficiency   \n",
       "3  Argument from Silence      1  informal  insufficiency   \n",
       "4  Argument from Silence      1  informal  insufficiency   \n",
       "\n",
       "                                     gpt_4o_response  \\\n",
       "0  No. The reasoning step is not necessarily corr...   \n",
       "1  No. The reasoning step is not correct. Bob's s...   \n",
       "2  No. The reasoning step is not correct. The con...   \n",
       "3  No. The reasoning step is not necessarily corr...   \n",
       "4  No. The reasoning step is not correct. The fac...   \n",
       "\n",
       "                          claude_3_5_sonnet_response  \\\n",
       "0  Let's think through this step-by-step:  1. Joh...   \n",
       "1  Let's think through this step by step:  1. Ali...   \n",
       "2  Let's think through this step-by-step:  1. Ali...   \n",
       "3  Let's think through this step-by-step:  1. The...   \n",
       "4  Let's think through this step-by-step:  1. Joh...   \n",
       "\n",
       "                             gemini_1_5_pro_response gpt_4o_mini_response  \\\n",
       "0  No.  Silence is not necessarily an admission o...                  No.   \n",
       "1  No.  Silence could indicate a lack of knowledg...                  No.   \n",
       "2  No.  Bob's silence doesn't confirm Alice's cla...                  No.   \n",
       "3  No.  While Tom's experience is valuable, the a...                  No.   \n",
       "4  No.  Silence does not imply guilt or fear.  Ma...                  No.   \n",
       "\n",
       "                             claude_3_haiku_response  \\\n",
       "0  Let's analyze this step-by-step:  1. John aske...   \n",
       "1  Let's analyze this step-by-step:  1. Alice ask...   \n",
       "2  Let's analyze this step-by-step:  1. Alice cla...   \n",
       "3  Let's analyze this step-by-step:  1. Tom, a se...   \n",
       "4  Let's analyze this step-by-step:  1. John accu...   \n",
       "\n",
       "                           gemini_1_5_flash_response  \\\n",
       "0  No.  Silence doesn't necessarily mean guilt or...   \n",
       "1  No.  Silence doesn't imply knowledge.  Bob's s...   \n",
       "2  No.  Bob's silence doesn't provide evidence su...   \n",
       "3  No.  The reasoning commits an appeal to author...   \n",
       "4  No.  Silence does not equal guilt or fear.  Ma...   \n",
       "\n",
       "                        gemini_1_5_flash_8b_response  \\\n",
       "0  No.  Silence doesn't necessarily mean agreemen...   \n",
       "1  No.  Silence does not equate to knowledge.  Bo...   \n",
       "2  No.  Alice's claim, even if true, and Bob's si...   \n",
       "3  No.  Just because one person says something an...   \n",
       "4  No.  Silence in the face of an accusation does...   \n",
       "\n",
       "                              llama_3_1_70b_response  \\\n",
       "0  To evaluate the correctness of the reasoning s...   \n",
       "1  No, the reasoning step is not correct.   The c...   \n",
       "2  To evaluate the correctness of the reasoning s...   \n",
       "3  To evaluate the correctness of the reasoning s...   \n",
       "4  No, the reasoning step is not correct.   Here'...   \n",
       "\n",
       "                               llama_3_1_8b_response  \\\n",
       "0  No.  This reasoning step is incorrect because ...   \n",
       "1  No.  This reasoning is incorrect because remai...   \n",
       "2  No.  This reasoning is incorrect because it as...   \n",
       "3  No.  The reasoning step is incorrect for sever...   \n",
       "4  No.  This reasoning step is incorrect because ...   \n",
       "\n",
       "                            mistral_large_2_response  \\\n",
       "0  No, the reasoning step is not necessarily corr...   \n",
       "1  No. Bob's silence does not necessarily indicat...   \n",
       "2  No, the reasoning step is not correct. Bob's s...   \n",
       "3  No, the reasoning step is not necessarily corr...   \n",
       "4  No. Mary's silence does not necessarily mean s...   \n",
       "\n",
       "                            mistral_small_2_response  \n",
       "0  No, the reasoning step is not correct. Maria's...  \n",
       "1  No, the reasoning step is not correct. Bob's s...  \n",
       "2  No, the reasoning step is not correct. Just be...  \n",
       "3  No, the reasoning step is not correct. Here's ...  \n",
       "4  No, the reasoning step is not correct. Just be...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_e13 = 'data/fallacies_e13.csv'\n",
    "df_fallacies_e13 = get_fallacy_df(filename_e13)\n",
    "df_fallacies_e13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38db9bd6d6849720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T07:22:53.846619Z",
     "start_time": "2024-11-01T07:22:53.825991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following reasoning step correct?\n",
      "Let's think step by step and then answer \"Yes\" or \"No\".\n",
      "[step]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_e13 = get_identification_cot_prompt_template()\n",
    "print(prompt_template_e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c8bd26ad1fafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T07:23:20.350485Z",
     "start_time": "2024-11-01T07:22:54.785734Z"
    }
   },
   "outputs": [],
   "source": [
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "run_experiment(df_fallacies_e13, filename_e13, prompt_template_e13, llms, sleep_seconds=0.2)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e13, filename_e13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484219cbdf024d",
   "metadata": {},
   "source": [
    "### Experiment 1.4: Fallacy Identification with Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2103f4702435f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T09:24:54.255277Z",
     "start_time": "2024-11-23T09:24:54.200531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-23 10:24:54] Loaded existing fallacy dataframe from data/fallacies_e14.csv.\n"
     ]
    }
   ],
   "source": [
    "filename_e14 = 'data/fallacies_e14.csv'\n",
    "df_fallacies_e14 = get_fallacy_df(filename_e14)\n",
    "\n",
    "# Select only test set\n",
    "df_fallacies_e14 = df_fallacies_e14[df_fallacies_e14['tuning'] == TuningSet.TEST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17052cf80dcd574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T09:24:54.878777Z",
     "start_time": "2024-11-23T09:24:54.861815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
      "[text]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_e14 = get_identification_zero_shot_prompt_template()\n",
    "print(prompt_template_e14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0c3af7ceab34b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:49:26.135049618Z",
     "start_time": "2024-11-05T17:14:04.400242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 18:15:13] Processed 100 responses for LLM gpt_4o_mini_identification (index=199).\n",
      "[2024-11-05 18:16:24] Processed 200 responses for LLM gpt_4o_mini_identification (index=399).\n",
      "[2024-11-05 18:17:34] Processed 300 responses for LLM gpt_4o_mini_identification (index=599).\n",
      "[2024-11-05 18:18:50] Processed 400 responses for LLM gpt_4o_mini_identification (index=799).\n",
      "[2024-11-05 18:19:58] Processed 500 responses for LLM gpt_4o_mini_identification (index=999).\n",
      "[2024-11-05 18:21:05] Processed 600 responses for LLM gpt_4o_mini_identification (index=1199).\n",
      "[2024-11-05 18:22:15] Processed 700 responses for LLM gpt_4o_mini_identification (index=1399).\n",
      "[2024-11-05 18:23:23] Processed 800 responses for LLM gpt_4o_mini_identification (index=1599).\n",
      "[2024-11-05 18:24:28] Processed 900 responses for LLM gpt_4o_mini_identification (index=1799).\n",
      "[2024-11-05 18:25:35] Processed 1000 responses for LLM gpt_4o_mini_identification (index=1999).\n",
      "[2024-11-05 18:26:43] Processed 1100 responses for LLM gpt_4o_mini_identification (index=2199).\n",
      "[2024-11-05 18:27:46] Processed 1200 responses for LLM gpt_4o_mini_identification (index=2399).\n",
      "[2024-11-05 18:28:51] Processed 1300 responses for LLM gpt_4o_mini_identification (index=2599).\n",
      "[2024-11-05 18:29:56] Processed 1400 responses for LLM gpt_4o_mini_identification (index=2799).\n",
      "[2024-11-05 18:30:59] Processed 1500 responses for LLM gpt_4o_mini_identification (index=2999).\n",
      "[2024-11-05 18:32:04] Processed 1600 responses for LLM gpt_4o_mini_identification (index=3199).\n",
      "[2024-11-05 18:33:09] Processed 1700 responses for LLM gpt_4o_mini_identification (index=3399).\n",
      "[2024-11-05 18:34:16] Processed 1800 responses for LLM gpt_4o_mini_identification (index=3599).\n",
      "[2024-11-05 18:35:24] Processed 1900 responses for LLM gpt_4o_mini_identification (index=3799).\n",
      "[2024-11-05 18:36:27] Processed 2000 responses for LLM gpt_4o_mini_identification (index=3999).\n",
      "[2024-11-05 18:37:30] Processed 2100 responses for LLM gpt_4o_mini_identification (index=4199).\n",
      "[2024-11-05 18:38:32] Processed 2200 responses for LLM gpt_4o_mini_identification (index=4399).\n",
      "[2024-11-05 18:39:35] Processed 2300 responses for LLM gpt_4o_mini_identification (index=4599).\n"
     ]
    }
   ],
   "source": [
    "llms = get_llms([LLM.GPT_4O_MINI_IDENTIFICATION])\n",
    "\n",
    "run_experiment(df_fallacies_e14, filename_e14, prompt_template_e14, llms, sleep_seconds=0.1)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e14, filename_e14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd71bd4a922ab8e",
   "metadata": {},
   "source": [
    "### Experiment 1.5: Fallacy Identification with Rephrased Prompt\n",
    "\n",
    "By rephrasing the prompt from \"Is the following reasoning step correct?\" to \"Does the following reasoning step contain a logical fallacy?\" we aim to reduce false positives. The LLMs might question the premises of the reasoning steps which may lead to the conclusion that a reasoning step is incorrect based on questionable premises, even though there is no logical fallacy.\n",
    "\n",
    "Correctness can be interpreted as depending both on validity (based on logical form) and soundness (depending on truth of the premises). In contrast, fallacies are about structural flaws, not the factual accuracy of premises. Therefore, LLMs might generate less false positives with the rephrased prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2fa5c386993372de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T22:08:03.044579Z",
     "start_time": "2025-01-30T22:08:02.619931Z"
    }
   },
   "source": [
    "filename_e15 = 'data/fallacies_e15.csv'\n",
    "df_fallacies_e15 = get_fallacy_df(filename_e15)\n",
    "df_fallacies_e15.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-30 23:08:03] Loaded existing fallacy dataframe from data/fallacies_e15.csv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy label  category    subcategory  \\\n",
       "0  Argument from Silence     1  informal  insufficiency   \n",
       "1  Argument from Silence     1  informal  insufficiency   \n",
       "2  Argument from Silence     1  informal  insufficiency   \n",
       "3  Argument from Silence     1  informal  insufficiency   \n",
       "4  Argument from Silence     1  informal  insufficiency   \n",
       "\n",
       "  claude_3_5_sonnet_response gpt_4o_response gemini_1_5_flash_response  \\\n",
       "0                        Yes             Yes                       Yes   \n",
       "1                        Yes             Yes                       Yes   \n",
       "2                        Yes             Yes                       Yes   \n",
       "3                        Yes             Yes                       Yes   \n",
       "4                        Yes             Yes                       Yes   \n",
       "\n",
       "   claude_3_5_sonnet_pred  claude_3_5_sonnet_score  gpt_4o_pred  gpt_4o_score  \\\n",
       "0                       1                        1            1             1   \n",
       "1                       1                        1            1             1   \n",
       "2                       1                        1            1             1   \n",
       "3                       1                        1            1             1   \n",
       "4                       1                        1            1             1   \n",
       "\n",
       "   gemini_1_5_flash_pred  gemini_1_5_flash_score  \n",
       "0                      1                       1  \n",
       "1                      1                       1  \n",
       "2                      1                       1  \n",
       "3                      1                       1  \n",
       "4                      1                       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gemini_1_5_flash_response</th>\n",
       "      <th>claude_3_5_sonnet_pred</th>\n",
       "      <th>claude_3_5_sonnet_score</th>\n",
       "      <th>gpt_4o_pred</th>\n",
       "      <th>gpt_4o_score</th>\n",
       "      <th>gemini_1_5_flash_pred</th>\n",
       "      <th>gemini_1_5_flash_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7d1901d2fa130803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T22:08:05.601702Z",
     "start_time": "2025-01-30T22:08:05.581107Z"
    }
   },
   "source": [
    "prompt_template_e15 = get_identification_zero_shot_prompt_template_v2()\n",
    "print(prompt_template_e15)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following reasoning step contain a logical fallacy? You can only answer \"Yes\" or \"No\".\n",
      "[text]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1e9aea0337fba767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T09:46:34.610023Z",
     "start_time": "2025-01-30T22:08:31.000807Z"
    }
   },
   "source": [
    "llms = get_llms([LLM.DEEPSEEK_R1_14B])\n",
    "\n",
    "run_experiment(df_fallacies_e15, filename_e15, prompt_template_e15, llms)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e15, filename_e15)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-31 00:13:25] Processed 100 responses for LLM deepseek_r1_14b (index=99).\n",
      "[2025-01-31 01:13:51] Processed 200 responses for LLM deepseek_r1_14b (index=199).\n",
      "[2025-01-31 02:19:54] Processed 300 responses for LLM deepseek_r1_14b (index=299).\n",
      "[2025-01-31 03:17:00] Processed 400 responses for LLM deepseek_r1_14b (index=399).\n",
      "[2025-01-31 04:14:12] Processed 500 responses for LLM deepseek_r1_14b (index=499).\n",
      "[2025-01-31 05:26:00] Processed 600 responses for LLM deepseek_r1_14b (index=599).\n",
      "[2025-01-31 06:30:38] Processed 700 responses for LLM deepseek_r1_14b (index=699).\n",
      "[2025-01-31 07:38:18] Processed 800 responses for LLM deepseek_r1_14b (index=799).\n",
      "[2025-01-31 08:45:22] Processed 900 responses for LLM deepseek_r1_14b (index=899).\n",
      "[2025-01-31 09:48:25] Processed 1000 responses for LLM deepseek_r1_14b (index=999).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m llms \u001B[38;5;241m=\u001B[39m get_llms([LLM\u001B[38;5;241m.\u001B[39mDEEPSEEK_R1_14B])\n\u001B[0;32m----> 3\u001B[0m run_experiment(df_fallacies_e15, filename_e15, prompt_template_e15, llms)\n\u001B[1;32m      5\u001B[0m save_fallacy_df(df_fallacies_e15, filename_e15)\n",
      "File \u001B[0;32m~/HSLU/Thesis/fallacy-detection/src/experiment.py:44\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(df, filename, prompt_template, llms, keep_existing_responses, sleep_seconds, log_responses, is_search)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m prompt_template\u001B[38;5;241m.\u001B[39mreplace(TEXT_PLACEHOLDER, row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 44\u001B[0m     response: AIMessage \u001B[38;5;241m=\u001B[39m chat_model\u001B[38;5;241m.\u001B[39minvoke(prompt)\n\u001B[1;32m     46\u001B[0m     response_text \u001B[38;5;241m=\u001B[39m _filter_response_text(response\u001B[38;5;241m.\u001B[39mcontent)\n\u001B[1;32m     47\u001B[0m     df\u001B[38;5;241m.\u001B[39mat[index, response_column] \u001B[38;5;241m=\u001B[39m response_text\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    275\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    280\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    281\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    283\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 284\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    285\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    286\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    287\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    288\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    289\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    290\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    291\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    292\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    293\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    294\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:791\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    783\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    785\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    788\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    789\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    790\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 791\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:638\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    636\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    637\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 638\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    639\u001B[0m                 m,\n\u001B[1;32m    640\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    641\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    642\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    643\u001B[0m             )\n\u001B[1;32m    644\u001B[0m         )\n\u001B[1;32m    645\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    646\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:856\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 856\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    857\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    858\u001B[0m         )\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_ollama/chat_models.py:701\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[1;32m    695\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    696\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    700\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[0;32m--> 701\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chat_stream_with_aggregation(\n\u001B[1;32m    702\u001B[0m         messages, stop, run_manager, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    703\u001B[0m     )\n\u001B[1;32m    704\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m final_chunk\u001B[38;5;241m.\u001B[39mgeneration_info\n\u001B[1;32m    705\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[1;32m    706\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(\n\u001B[1;32m    707\u001B[0m             content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    711\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[1;32m    712\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_ollama/chat_models.py:602\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    595\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    599\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    600\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[1;32m    601\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 602\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m stream_resp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_stream(messages, stop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stream_resp, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    604\u001B[0m             chunk \u001B[38;5;241m=\u001B[39m ChatGenerationChunk(\n\u001B[1;32m    605\u001B[0m                 message\u001B[38;5;241m=\u001B[39mAIMessageChunk(\n\u001B[1;32m    606\u001B[0m                     content\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    619\u001B[0m                 ),\n\u001B[1;32m    620\u001B[0m             )\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/langchain_ollama/chat_models.py:589\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[0;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[1;32m    586\u001B[0m chat_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chat_params(messages, stop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chat_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 589\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mchat_params)\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mchat_params)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/ollama/_client.py:170\u001B[0m, in \u001B[0;36mClient._request.<locals>.inner\u001B[0;34m()\u001B[0m\n\u001B[1;32m    167\u001B[0m   e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    168\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 170\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_lines():\n\u001B[1;32m    171\u001B[0m   part \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(line)\n\u001B[1;32m    172\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m err \u001B[38;5;241m:=\u001B[39m part\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_models.py:861\u001B[0m, in \u001B[0;36mResponse.iter_lines\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    859\u001B[0m decoder \u001B[38;5;241m=\u001B[39m LineDecoder()\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 861\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_text():\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m decoder\u001B[38;5;241m.\u001B[39mdecode(text):\n\u001B[1;32m    863\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m line\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_models.py:848\u001B[0m, in \u001B[0;36mResponse.iter_text\u001B[0;34m(self, chunk_size)\u001B[0m\n\u001B[1;32m    846\u001B[0m chunker \u001B[38;5;241m=\u001B[39m TextChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    847\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 848\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m byte_content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_bytes():\n\u001B[1;32m    849\u001B[0m         text_content \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39mdecode(byte_content)\n\u001B[1;32m    850\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker\u001B[38;5;241m.\u001B[39mdecode(text_content):\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_models.py:829\u001B[0m, in \u001B[0;36mResponse.iter_bytes\u001B[0;34m(self, chunk_size)\u001B[0m\n\u001B[1;32m    827\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 829\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m raw_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_raw():\n\u001B[1;32m    830\u001B[0m         decoded \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39mdecode(raw_bytes)\n\u001B[1;32m    831\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker\u001B[38;5;241m.\u001B[39mdecode(decoded):\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_models.py:883\u001B[0m, in \u001B[0;36mResponse.iter_raw\u001B[0;34m(self, chunk_size)\u001B[0m\n\u001B[1;32m    880\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    882\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 883\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m raw_stream_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream:\n\u001B[1;32m    884\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_bytes_downloaded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(raw_stream_bytes)\n\u001B[1;32m    885\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker\u001B[38;5;241m.\u001B[39mdecode(raw_stream_bytes):\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_client.py:126\u001B[0m, in \u001B[0;36mBoundSyncStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n\u001B[1;32m    127\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpx/_transports/default.py:113\u001B[0m, in \u001B[0;36mResponseStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 113\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_httpcore_stream:\n\u001B[1;32m    114\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:361\u001B[0m, in \u001B[0;36mConnectionPoolByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m--> 361\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_sync/http11.py:337\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 337\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_sync/http11.py:329\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_body\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request, kwargs):\n\u001B[0;32m--> 329\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39m_receive_response_body(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    330\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    332\u001B[0m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_sync/http11.py:198\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_body\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    195\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 198\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_receive_event(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mData):\n\u001B[1;32m    200\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    209\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 212\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\u001B[38;5;241m.\u001B[39mread(\n\u001B[1;32m    213\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mREAD_NUM_BYTES, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    214\u001B[0m     )\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/miniconda3/envs/hslu-thesis/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv(max_bytes)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "33880d85093a00a0",
   "metadata": {},
   "source": [
    "### Experiment 1.6: Fallacy Identification Reliability Test\n",
    "\n",
    "Are the results from the previous experiment replicable? Since temperature is set to 0, the models should produce the same output for the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29fd7d0f1dd4c0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:48:48.530695Z",
     "start_time": "2024-11-11T12:48:48.494448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 13:48:48] Loaded existing fallacy dataframe from data/fallacies_e16.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy label  category    subcategory gpt_4o_mini_response  \n",
       "0  Argument from Silence     1  informal  insufficiency                  No.  \n",
       "1  Argument from Silence     1  informal  insufficiency                  No.  \n",
       "2  Argument from Silence     1  informal  insufficiency                  No.  \n",
       "3  Argument from Silence     1  informal  insufficiency                  No.  \n",
       "4  Argument from Silence     1  informal  insufficiency                  No.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_e16 = 'data/fallacies_e16.csv'\n",
    "df_fallacies_e16 = get_fallacy_df(filename_e16)\n",
    "df_fallacies_e16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b024ce81f434afcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:49:06.356126Z",
     "start_time": "2024-11-11T12:49:06.338095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following reasoning step correct? You can only answer \"Yes\" or \"No\".\n",
      "[step]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_e16 = get_identification_zero_shot_prompt_template()\n",
    "print(prompt_template_e16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2019da909560b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T14:29:23.626749Z",
     "start_time": "2024-11-11T12:49:07.052388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 13:50:07] Processed 100 responses for LLM gpt_4o_mini (index=289).\n",
      "[2024-11-11 13:51:01] Processed 200 responses for LLM gpt_4o_mini (index=389).\n",
      "[2024-11-11 13:51:50] Processed 300 responses for LLM gpt_4o_mini (index=489).\n",
      "[2024-11-11 13:52:37] Processed 400 responses for LLM gpt_4o_mini (index=589).\n",
      "[2024-11-11 13:53:21] Processed 500 responses for LLM gpt_4o_mini (index=689).\n",
      "[2024-11-11 13:54:12] Processed 600 responses for LLM gpt_4o_mini (index=789).\n",
      "[2024-11-11 13:55:00] Processed 700 responses for LLM gpt_4o_mini (index=889).\n",
      "[2024-11-11 13:55:51] Processed 800 responses for LLM gpt_4o_mini (index=989).\n",
      "[2024-11-11 13:56:38] Processed 900 responses for LLM gpt_4o_mini (index=1089).\n",
      "[2024-11-11 13:57:24] Processed 1000 responses for LLM gpt_4o_mini (index=1189).\n",
      "[2024-11-11 13:58:13] Processed 1100 responses for LLM gpt_4o_mini (index=1289).\n",
      "[2024-11-11 13:59:01] Processed 1200 responses for LLM gpt_4o_mini (index=1389).\n",
      "[2024-11-11 13:59:52] Processed 1300 responses for LLM gpt_4o_mini (index=1489).\n",
      "[2024-11-11 14:00:37] Processed 1400 responses for LLM gpt_4o_mini (index=1589).\n",
      "[2024-11-11 14:01:24] Processed 1500 responses for LLM gpt_4o_mini (index=1689).\n",
      "[2024-11-11 14:02:12] Processed 1600 responses for LLM gpt_4o_mini (index=1789).\n",
      "[2024-11-11 14:03:00] Processed 1700 responses for LLM gpt_4o_mini (index=1889).\n",
      "[2024-11-11 14:03:59] Processed 1800 responses for LLM gpt_4o_mini (index=1989).\n",
      "[2024-11-11 14:04:46] Processed 1900 responses for LLM gpt_4o_mini (index=2089).\n",
      "[2024-11-11 14:05:34] Processed 2000 responses for LLM gpt_4o_mini (index=2189).\n",
      "[2024-11-11 14:06:22] Processed 2100 responses for LLM gpt_4o_mini (index=2289).\n",
      "[2024-11-11 14:07:09] Processed 2200 responses for LLM gpt_4o_mini (index=2389).\n",
      "[2024-11-11 14:07:55] Processed 2300 responses for LLM gpt_4o_mini (index=2489).\n",
      "[2024-11-11 14:08:44] Processed 2400 responses for LLM gpt_4o_mini (index=2589).\n",
      "[2024-11-11 14:09:34] Processed 2500 responses for LLM gpt_4o_mini (index=2689).\n",
      "[2024-11-11 14:10:26] Processed 2600 responses for LLM gpt_4o_mini (index=2789).\n",
      "[2024-11-11 14:11:17] Processed 2700 responses for LLM gpt_4o_mini (index=2889).\n",
      "[2024-11-11 14:12:13] Processed 2800 responses for LLM gpt_4o_mini (index=2989).\n",
      "[2024-11-11 14:13:01] Processed 2900 responses for LLM gpt_4o_mini (index=3089).\n",
      "[2024-11-11 14:13:47] Processed 3000 responses for LLM gpt_4o_mini (index=3189).\n",
      "[2024-11-11 14:14:33] Processed 3100 responses for LLM gpt_4o_mini (index=3289).\n",
      "[2024-11-11 14:15:28] Processed 3200 responses for LLM gpt_4o_mini (index=3389).\n",
      "[2024-11-11 14:16:13] Processed 3300 responses for LLM gpt_4o_mini (index=3489).\n",
      "[2024-11-11 14:17:01] Processed 3400 responses for LLM gpt_4o_mini (index=3589).\n",
      "[2024-11-11 14:17:59] Processed 3500 responses for LLM gpt_4o_mini (index=3689).\n",
      "[2024-11-11 14:18:51] Processed 3600 responses for LLM gpt_4o_mini (index=3789).\n",
      "[2024-11-11 14:19:37] Processed 3700 responses for LLM gpt_4o_mini (index=3889).\n",
      "[2024-11-11 14:20:23] Processed 3800 responses for LLM gpt_4o_mini (index=3989).\n",
      "[2024-11-11 14:21:15] Processed 3900 responses for LLM gpt_4o_mini (index=4089).\n",
      "[2024-11-11 14:22:03] Processed 4000 responses for LLM gpt_4o_mini (index=4189).\n",
      "[2024-11-11 14:22:51] Processed 4100 responses for LLM gpt_4o_mini (index=4289).\n",
      "[2024-11-11 14:23:43] Processed 4200 responses for LLM gpt_4o_mini (index=4389).\n",
      "[2024-11-11 14:24:33] Processed 4300 responses for LLM gpt_4o_mini (index=4489).\n",
      "[2024-11-11 14:25:21] Processed 4400 responses for LLM gpt_4o_mini (index=4589).\n",
      "[2024-11-11 14:26:26] Processed 100 responses for LLM claude_3_haiku (index=99).\n",
      "[2024-11-11 14:27:07] Processed 200 responses for LLM claude_3_haiku (index=199).\n",
      "[2024-11-11 14:27:49] Processed 300 responses for LLM claude_3_haiku (index=299).\n",
      "[2024-11-11 14:28:31] Processed 400 responses for LLM claude_3_haiku (index=399).\n",
      "[2024-11-11 14:29:12] Processed 500 responses for LLM claude_3_haiku (index=499).\n",
      "[2024-11-11 14:29:55] Processed 600 responses for LLM claude_3_haiku (index=599).\n",
      "[2024-11-11 14:30:35] Processed 700 responses for LLM claude_3_haiku (index=699).\n",
      "[2024-11-11 14:31:16] Processed 800 responses for LLM claude_3_haiku (index=799).\n",
      "[2024-11-11 14:32:01] Processed 900 responses for LLM claude_3_haiku (index=899).\n",
      "[2024-11-11 14:32:43] Processed 1000 responses for LLM claude_3_haiku (index=999).\n",
      "[2024-11-11 14:33:20] Processed 1100 responses for LLM claude_3_haiku (index=1099).\n",
      "[2024-11-11 14:33:59] Processed 1200 responses for LLM claude_3_haiku (index=1199).\n",
      "[2024-11-11 14:34:39] Processed 1300 responses for LLM claude_3_haiku (index=1299).\n",
      "[2024-11-11 14:35:20] Processed 1400 responses for LLM claude_3_haiku (index=1399).\n",
      "[2024-11-11 14:36:00] Processed 1500 responses for LLM claude_3_haiku (index=1499).\n",
      "[2024-11-11 14:36:41] Processed 1600 responses for LLM claude_3_haiku (index=1599).\n",
      "[2024-11-11 14:37:24] Processed 1700 responses for LLM claude_3_haiku (index=1699).\n",
      "[2024-11-11 14:38:03] Processed 1800 responses for LLM claude_3_haiku (index=1799).\n",
      "[2024-11-11 14:38:42] Processed 1900 responses for LLM claude_3_haiku (index=1899).\n",
      "[2024-11-11 14:39:19] Processed 2000 responses for LLM claude_3_haiku (index=1999).\n",
      "[2024-11-11 14:39:58] Processed 2100 responses for LLM claude_3_haiku (index=2099).\n",
      "[2024-11-11 14:40:37] Processed 2200 responses for LLM claude_3_haiku (index=2199).\n",
      "[2024-11-11 14:41:17] Processed 2300 responses for LLM claude_3_haiku (index=2299).\n",
      "[2024-11-11 14:41:59] Processed 2400 responses for LLM claude_3_haiku (index=2399).\n",
      "[2024-11-11 14:42:40] Processed 2500 responses for LLM claude_3_haiku (index=2499).\n",
      "[2024-11-11 14:43:18] Processed 2600 responses for LLM claude_3_haiku (index=2599).\n",
      "[2024-11-11 14:44:00] Processed 2700 responses for LLM claude_3_haiku (index=2699).\n",
      "[2024-11-11 14:44:41] Processed 2800 responses for LLM claude_3_haiku (index=2799).\n",
      "[2024-11-11 14:45:20] Processed 2900 responses for LLM claude_3_haiku (index=2899).\n",
      "[2024-11-11 14:46:00] Processed 3000 responses for LLM claude_3_haiku (index=2999).\n",
      "[2024-11-11 14:46:41] Processed 3100 responses for LLM claude_3_haiku (index=3099).\n",
      "[2024-11-11 14:47:23] Processed 3200 responses for LLM claude_3_haiku (index=3199).\n",
      "[2024-11-11 14:48:03] Processed 3300 responses for LLM claude_3_haiku (index=3299).\n",
      "[2024-11-11 14:48:44] Processed 3400 responses for LLM claude_3_haiku (index=3399).\n",
      "[2024-11-11 14:49:28] Processed 3500 responses for LLM claude_3_haiku (index=3499).\n",
      "[2024-11-11 14:50:08] Processed 3600 responses for LLM claude_3_haiku (index=3599).\n",
      "[2024-11-11 14:50:49] Processed 3700 responses for LLM claude_3_haiku (index=3699).\n",
      "[2024-11-11 14:51:30] Processed 3800 responses for LLM claude_3_haiku (index=3799).\n",
      "[2024-11-11 14:52:13] Processed 3900 responses for LLM claude_3_haiku (index=3899).\n",
      "[2024-11-11 14:52:54] Processed 4000 responses for LLM claude_3_haiku (index=3999).\n",
      "[2024-11-11 14:53:35] Processed 4100 responses for LLM claude_3_haiku (index=4099).\n",
      "[2024-11-11 14:54:18] Processed 4200 responses for LLM claude_3_haiku (index=4199).\n",
      "[2024-11-11 14:54:58] Processed 4300 responses for LLM claude_3_haiku (index=4299).\n",
      "[2024-11-11 14:55:40] Processed 4400 responses for LLM claude_3_haiku (index=4399).\n",
      "[2024-11-11 14:56:19] Processed 4500 responses for LLM claude_3_haiku (index=4499).\n",
      "[2024-11-11 14:57:00] Processed 4600 responses for LLM claude_3_haiku (index=4599).\n",
      "[2024-11-11 14:58:01] Processed 100 responses for LLM gemini_1_5_flash (index=99).\n",
      "[2024-11-11 14:58:40] Processed 200 responses for LLM gemini_1_5_flash (index=199).\n",
      "[2024-11-11 14:59:27] Processed 300 responses for LLM gemini_1_5_flash (index=299).\n",
      "[2024-11-11 15:00:06] Processed 400 responses for LLM gemini_1_5_flash (index=399).\n",
      "[2024-11-11 15:00:46] Processed 500 responses for LLM gemini_1_5_flash (index=499).\n",
      "[2024-11-11 15:01:27] Processed 600 responses for LLM gemini_1_5_flash (index=599).\n",
      "[2024-11-11 15:02:06] Processed 700 responses for LLM gemini_1_5_flash (index=699).\n",
      "[2024-11-11 15:02:47] Processed 800 responses for LLM gemini_1_5_flash (index=799).\n",
      "[2024-11-11 15:03:33] Processed 900 responses for LLM gemini_1_5_flash (index=899).\n",
      "[2024-11-11 15:04:15] Processed 1000 responses for LLM gemini_1_5_flash (index=999).\n",
      "[2024-11-11 15:04:56] Processed 1100 responses for LLM gemini_1_5_flash (index=1099).\n",
      "[2024-11-11 15:05:36] Processed 1200 responses for LLM gemini_1_5_flash (index=1199).\n",
      "[2024-11-11 15:06:17] Processed 1300 responses for LLM gemini_1_5_flash (index=1299).\n",
      "[2024-11-11 15:06:57] Processed 1400 responses for LLM gemini_1_5_flash (index=1399).\n",
      "[2024-11-11 15:07:38] Processed 1500 responses for LLM gemini_1_5_flash (index=1499).\n",
      "[2024-11-11 15:08:26] Processed 1600 responses for LLM gemini_1_5_flash (index=1599).\n",
      "[2024-11-11 15:09:06] Processed 1700 responses for LLM gemini_1_5_flash (index=1699).\n",
      "[2024-11-11 15:09:47] Processed 1800 responses for LLM gemini_1_5_flash (index=1799).\n",
      "[2024-11-11 15:10:27] Processed 1900 responses for LLM gemini_1_5_flash (index=1899).\n",
      "[2024-11-11 15:11:07] Processed 2000 responses for LLM gemini_1_5_flash (index=1999).\n",
      "[2024-11-11 15:11:48] Processed 2100 responses for LLM gemini_1_5_flash (index=2099).\n",
      "[2024-11-11 15:12:34] Processed 2200 responses for LLM gemini_1_5_flash (index=2199).\n",
      "[2024-11-11 15:13:17] Processed 2300 responses for LLM gemini_1_5_flash (index=2299).\n",
      "[2024-11-11 15:13:58] Processed 2400 responses for LLM gemini_1_5_flash (index=2399).\n",
      "[2024-11-11 15:14:38] Processed 2500 responses for LLM gemini_1_5_flash (index=2499).\n",
      "[2024-11-11 15:15:19] Processed 2600 responses for LLM gemini_1_5_flash (index=2599).\n",
      "[2024-11-11 15:15:59] Processed 2700 responses for LLM gemini_1_5_flash (index=2699).\n",
      "[2024-11-11 15:16:42] Processed 2800 responses for LLM gemini_1_5_flash (index=2799).\n",
      "[2024-11-11 15:17:28] Processed 2900 responses for LLM gemini_1_5_flash (index=2899).\n",
      "[2024-11-11 15:18:08] Processed 3000 responses for LLM gemini_1_5_flash (index=2999).\n",
      "[2024-11-11 15:18:47] Processed 3100 responses for LLM gemini_1_5_flash (index=3099).\n",
      "[2024-11-11 15:19:28] Processed 3200 responses for LLM gemini_1_5_flash (index=3199).\n",
      "[2024-11-11 15:20:09] Processed 3300 responses for LLM gemini_1_5_flash (index=3299).\n",
      "[2024-11-11 15:20:50] Processed 3400 responses for LLM gemini_1_5_flash (index=3399).\n",
      "[2024-11-11 15:21:32] Processed 3500 responses for LLM gemini_1_5_flash (index=3499).\n",
      "[2024-11-11 15:22:18] Processed 3600 responses for LLM gemini_1_5_flash (index=3599).\n",
      "[2024-11-11 15:22:59] Processed 3700 responses for LLM gemini_1_5_flash (index=3699).\n",
      "[2024-11-11 15:23:40] Processed 3800 responses for LLM gemini_1_5_flash (index=3799).\n",
      "[2024-11-11 15:24:19] Processed 3900 responses for LLM gemini_1_5_flash (index=3899).\n",
      "[2024-11-11 15:25:00] Processed 4000 responses for LLM gemini_1_5_flash (index=3999).\n",
      "[2024-11-11 15:25:40] Processed 4100 responses for LLM gemini_1_5_flash (index=4099).\n",
      "[2024-11-11 15:26:23] Processed 4200 responses for LLM gemini_1_5_flash (index=4199).\n",
      "[2024-11-11 15:27:03] Processed 4300 responses for LLM gemini_1_5_flash (index=4299).\n",
      "[2024-11-11 15:27:43] Processed 4400 responses for LLM gemini_1_5_flash (index=4399).\n",
      "[2024-11-11 15:28:26] Processed 4500 responses for LLM gemini_1_5_flash (index=4499).\n",
      "[2024-11-11 15:29:06] Processed 4600 responses for LLM gemini_1_5_flash (index=4599).\n"
     ]
    }
   ],
   "source": [
    "llms = get_llms([LLM.GPT_4O_MINI, LLM.GEMINI_1_5_FLASH, LLM.CLAUDE_3_HAIKU])\n",
    "\n",
    "run_experiment(df_fallacies_e16, filename_e16, prompt_template_e16, llms, sleep_seconds=0.0)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e16, filename_e16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873af238c00c0f56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127c2d47d3dc8918",
   "metadata": {},
   "source": [
    "## Fallacy Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54643504afccdcb8",
   "metadata": {},
   "source": [
    "### Experiment 2.1: Fallacy Classification with Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de2807d037c5b7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T07:38:53.819433Z",
     "start_time": "2024-11-01T07:38:53.775206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 08:38:53] Loaded existing fallacy dataframe from data/fallacies_e21.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>entity</th>\n",
       "      <th>fallacy</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4_response</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "      <th>claude_3_5_sonnet_response</th>\n",
       "      <th>claude_3_haiku_response</th>\n",
       "      <th>gemini_1_5_pro_response</th>\n",
       "      <th>gemini_1_5_flash_8b_response</th>\n",
       "      <th>o1_preview_response</th>\n",
       "      <th>mistral_large_2_response</th>\n",
       "      <th>mistral_small_2_response</th>\n",
       "      <th>llama_3_1_70b_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since John asked Maria if she used the last of...</td>\n",
       "      <td>tepas</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Affirming the Consequent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Denying the Antecedent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Alice asked if Bob knew what an 'ossia' ...</td>\n",
       "      <td>ossia</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Affirming the Consequent</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>(164) Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since Alice claims that the Hausdorff contents...</td>\n",
       "      <td>hausdorff contents</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Appeal to Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since Tom, a seasoned tugboater, said that ice...</td>\n",
       "      <td>tugboaters</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>(79) Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Appeal to Authority</td>\n",
       "      <td>Argument from Silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since John accuses Mary of being terrified of ...</td>\n",
       "      <td>beewolf</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>informal</td>\n",
       "      <td>insufficiency</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "      <td>Argument from Silence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                step              entity  \\\n",
       "0  Since John asked Maria if she used the last of...               tepas   \n",
       "1  Since Alice asked if Bob knew what an 'ossia' ...               ossia   \n",
       "2  Since Alice claims that the Hausdorff contents...  hausdorff contents   \n",
       "3  Since Tom, a seasoned tugboater, said that ice...          tugboaters   \n",
       "4  Since John accuses Mary of being terrified of ...             beewolf   \n",
       "\n",
       "                 fallacy  label  category    subcategory  \\\n",
       "0  Argument from Silence      1  informal  insufficiency   \n",
       "1  Argument from Silence      1  informal  insufficiency   \n",
       "2  Argument from Silence      1  informal  insufficiency   \n",
       "3  Argument from Silence      1  informal  insufficiency   \n",
       "4  Argument from Silence      1  informal  insufficiency   \n",
       "\n",
       "         gpt_4o_response            gpt_4_response      gpt_4o_mini_response  \\\n",
       "0  Argument from Silence     Argument from Silence  Affirming the Consequent   \n",
       "1  Argument from Silence     Argument from Silence  Affirming the Consequent   \n",
       "2  Argument from Silence     Argument from Silence     Argument from Silence   \n",
       "3  Argument from Silence  (79) Appeal to Authority       Appeal to Authority   \n",
       "4  Argument from Silence     Argument from Silence     Argument from Silence   \n",
       "\n",
       "  claude_3_5_sonnet_response claude_3_haiku_response gemini_1_5_pro_response  \\\n",
       "0      Argument from Silence  Denying the Antecedent   Argument from Silence   \n",
       "1      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "2      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "3        Appeal to Authority     Appeal to Authority     Appeal to Authority   \n",
       "4      Argument from Silence   Argument from Silence   Argument from Silence   \n",
       "\n",
       "  gemini_1_5_flash_8b_response    o1_preview_response  \\\n",
       "0        Argument from Silence  Argument from Silence   \n",
       "1        Argument from Silence  Argument from Silence   \n",
       "2        Argument from Silence  Argument from Silence   \n",
       "3          Appeal to Authority  Argument from Silence   \n",
       "4        Argument from Silence  Argument from Silence   \n",
       "\n",
       "  mistral_large_2_response     mistral_small_2_response llama_3_1_70b_response  \n",
       "0    Argument from Silence        Argument from Silence  Argument from Silence  \n",
       "1    Argument from Silence  (164) Argument from Silence  Argument from Silence  \n",
       "2    Argument from Silence            Appeal to Silence  Argument from Silence  \n",
       "3    Argument from Silence          Appeal to Authority  Argument from Silence  \n",
       "4    Argument from Silence        Argument from Silence  Argument from Silence  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_21 = 'data/fallacies_e21.csv'\n",
    "df_fallacies_e21 = get_fallacy_df(filename_21, only_incorrect=True)\n",
    "\n",
    "df_fallacies_e21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a36cf979639663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T07:38:54.254846Z",
     "start_time": "2024-11-01T07:38:54.228088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a logical fallacy classifier. Given an incorrect reasoning step, your task is to identify its type of fallacy.\n",
      "Answer by choosing one of these fallacies:\n",
      "(1) Affirming the Consequent\n",
      "(2) Denying the Antecedent\n",
      "(3) Negating Antecedent and Consequent\n",
      "(4) Commutation of Conditionals\n",
      "(5) Affirming a Disjunct\n",
      "(6) Denying a Conjunct\n",
      "(7) Fallacy of the Undistributed Middle\n",
      "(8) Exclusive Premises\n",
      "(9) Fallacy of Four Terms\n",
      "(10) Illicit Substitution of Identicals\n",
      "(11) Illicit Minor\n",
      "(12) Illicit Major\n",
      "(13) Negative Conclusion from Affirmative Premises\n",
      "(14) Affirmative Conclusion from a Negative Premise\n",
      "(15) False Conversion\n",
      "(16) Unwarranted Contrast\n",
      "(17) Quantifier Shift Fallacy\n",
      "(18) Existential Fallacy\n",
      "(19) Fallacy of Every and All\n",
      "(20) Illicit Contraposition\n",
      "(21) Gamblers Fallacy\n",
      "(22) Hot Hand Fallacy\n",
      "(23) Conjunction Fallacy\n",
      "(24) Disjunction Fallacy\n",
      "(25) Argument of the Beard\n",
      "(26) Appeal to Extremes\n",
      "(27) Type Token Fallacy\n",
      "(28) Use Mention Error\n",
      "(29) Reification\n",
      "(30) Fake Precision\n",
      "(31) No True Scotsman\n",
      "(32) Contextomy\n",
      "(33) Stolen Concept Fallacy\n",
      "(34) Anthropomorphism\n",
      "(35) Accent Fallacy\n",
      "(36) Ambiguity Fallacy\n",
      "(37) Alphabet Soup\n",
      "(38) Equivocation\n",
      "(39) Modal Scope Fallacy\n",
      "(40) Inconsistency\n",
      "(41) Conflicting Conditions\n",
      "(42) Kettle Logic\n",
      "(43) Political Correctness Fallacy\n",
      "(44) Appeal to Complexity\n",
      "(45) Statement of Conversion\n",
      "(46) Appeal to the Moon\n",
      "(47) Quantum Physics Fallacy\n",
      "(48) fact to fiction fallacy\n",
      "(49) Non Sequitur\n",
      "(50) Inflation of Conflict\n",
      "(51) Argument by Fast Talking\n",
      "(52) Appeal to Intuition\n",
      "(53) Appeal to Closure\n",
      "(54) Appeal to Definition\n",
      "(55) Spiritual Fallacy\n",
      "(56) gish gallop\n",
      "(57) Denying the Correlative\n",
      "(58) Red Herring\n",
      "(59) Strawman Fallacy\n",
      "(60) Avoiding the Issue\n",
      "(61) Logic Chopping\n",
      "(62) Meaningless Question\n",
      "(63) Failure to Elucidate\n",
      "(64) Argument by Gibberish\n",
      "(65) Hypnotic Bait and Switch\n",
      "(66) Traitorous Critic Fallacy\n",
      "(67) Having Your Cake\n",
      "(68) Appeal to Common Belief\n",
      "(69) Appeal to Popularity\n",
      "(70) Appeal to Common Sense\n",
      "(71) Appeal to Common Folk\n",
      "(72) Appeal to Trust\n",
      "(73) Argument from Age\n",
      "(74) Appeal to Heaven\n",
      "(75) Appeal to Tradition\n",
      "(76) Etymological Fallacy\n",
      "(77) Genetic Fallacy\n",
      "(78) Appeal to Celebrity\n",
      "(79) Appeal to Authority\n",
      "(80) Appeal to False Authority\n",
      "(81) Argument from False Authority\n",
      "(82) Blind Authority Fallacy\n",
      "(83) Argument by Personal Charm\n",
      "(84) Argument to the Purse\n",
      "(85) Ad Hominem Circumstantial\n",
      "(86) Gadarene Swine Fallacy\n",
      "(87) Ad Hominem Tu quoque\n",
      "(88) Bulverism\n",
      "(89) Righteousness Fallacy\n",
      "(90) Self Righteousness Fallacy\n",
      "(91) Reductio ad Hitlerum\n",
      "(92) Ad Hominem Guilt by Association\n",
      "(93) Identity Fallacy\n",
      "(94) Appeal to Stupidity\n",
      "(95) Ad Hominem Abusive\n",
      "(96) Ad Fidentia\n",
      "(97) appeal to loyalty\n",
      "(98) Appeal to Accomplishment\n",
      "(99) Scapegoating\n",
      "(100) Fallacy of Opposition\n",
      "(101) Proof by Intimidation\n",
      "(102) Poisoning the Well\n",
      "(103) Wishful Thinking\n",
      "(104) Appeal to Faith\n",
      "(105) Notable Effort\n",
      "(106) Prejudicial Language\n",
      "(107) Special Pleading\n",
      "(108) If By Whiskey\n",
      "(109) Overextended Outrage\n",
      "(110) Appeal to Ridicule\n",
      "(111) Argument by Emotive Language\n",
      "(112) Style Over Substance\n",
      "(113) Appeal to Anger\n",
      "(114) Appeal to Pity\n",
      "(115) Appeal to Emotion\n",
      "(116) Appeal to Flattery\n",
      "(117) Appeal to Spite\n",
      "(118) pragmatic fallacy\n",
      "(119) Appeal to Force\n",
      "(120) Appeal to Fear\n",
      "(121) Fallacy of Composition\n",
      "(122) Fallacy of Division\n",
      "(123) Stereotyping the fallacy\n",
      "(124) Ecological Fallacy\n",
      "(125) Oversimplified Cause Fallacy\n",
      "(126) Accident Fallacy\n",
      "(127) mcnamara fallacy\n",
      "(128) Overwhelming Exception\n",
      "(129) Reductio ad Absurdum\n",
      "(130) Nirvana Fallacy\n",
      "(131) Relative Privation\n",
      "(132) imposter fallacy\n",
      "(133) Misleading Vividness\n",
      "(134) Appeal to Possibility\n",
      "(135) Rights To Ought Fallacy\n",
      "(136) Psychogenetic Fallacy\n",
      "(137) Weak Analogy\n",
      "(138) Extended Analogy\n",
      "(139) Appeal to Equality\n",
      "(140) False Equivalence\n",
      "(141) Galileo Fallacy\n",
      "(142) Post Designation\n",
      "(143) Just In Case Fallacy\n",
      "(144) Selective Attention\n",
      "(145) nutpicking fallacy\n",
      "(146) Biased Sample Fallacy\n",
      "(147) Survivorship Fallacy\n",
      "(148) Spotlight Fallacy\n",
      "(149) Hasty Generalization\n",
      "(150) Incomplete Comparison\n",
      "(151) Texas Sharpshooter Fallacy\n",
      "(152) Faulty Comparison\n",
      "(153) Base Rate Fallacy\n",
      "(154) Least Plausible Hypothesis\n",
      "(155) Far Fetched Hypothesis\n",
      "(156) Cherry Picking\n",
      "(157) Argument by Selective Reading\n",
      "(158) deceptive sharing\n",
      "(159) Multiple Comparisons Fallacy\n",
      "(160) Magical Thinking\n",
      "(161) Slippery Slope\n",
      "(162) Sunk Cost Fallacy\n",
      "(163) Jumping to Conclusions\n",
      "(164) Argument from Silence\n",
      "(165) Argument from Hearsay\n",
      "(166) Anonymous Authority\n",
      "(167) Insignificant Cause\n",
      "(168) Just Because Fallacy\n",
      "(169) Appeal to the Law\n",
      "(170) Appeal to Normality\n",
      "(171) False Effect\n",
      "(172) Appeal to Consequences\n",
      "(173) Retrogressive Causation\n",
      "(174) Confusing Currently Unexplained with Unexplainable\n",
      "(175) Appeal to Desperation\n",
      "(176) Regression Fallacy\n",
      "(177) Causal Reductionism\n",
      "(178) Questionable Cause\n",
      "(179) Hedging\n",
      "(180) Circular Definition\n",
      "(181) Homunculus Fallacy\n",
      "(182) Circular Reasoning\n",
      "(183) Tokenism\n",
      "(184) Appeal to Novelty\n",
      "(185) Two Wrongs Make a Right\n",
      "(186) Appeal to Nature\n",
      "(187) Naturalistic Fallacy\n",
      "(188) Moralistic Fallacy\n",
      "(189) Suppressed Correlative\n",
      "(190) Historians Fallacy\n",
      "(191) Willed Ignorance\n",
      "(192) Appeal to Coincidence\n",
      "(193) Argument from Incredulity\n",
      "(194) Argument by Pigheadedness\n",
      "(195) Argument by Repetition\n",
      "(196) Definist Fallacy\n",
      "(197) Limited Scope\n",
      "(198) Moving the Goalposts\n",
      "(199) Argument from Fallacy\n",
      "(200) False Dilemma\n",
      "(201) Argument from Ignorance\n",
      "(202) Alternative Advance\n",
      "(203) Shifting of the Burden of Proof\n",
      "(204) Proving Non Existence\n",
      "(205) Proof Surrogate\n",
      "(206) Rationalization\n",
      "(207) Spin Doctoring\n",
      "(208) Lying with Statistics\n",
      "(209) Ad Hoc Rescue\n",
      "(210) False Attribution\n",
      "(211) Amazing Familiarity\n",
      "(212) Ludic Fallacy\n",
      "(213) Missing Data Fallacy\n",
      "(214) Begging the Question\n",
      "(215) Complex Question Fallacy\n",
      "(216) Package Deal Fallacy\n",
      "(217) Subjectivist Fallacy\n",
      "(218) Distinction Without a Difference\n",
      "(219) Hypothesis Contrary to Fact\n",
      "(220) Shoehorning\n",
      "(221) Appeal to Self evident Truth\n",
      "(222) Subverted Support\n",
      "(223) Double Standard\n",
      "(224) Fantasy Projection\n",
      "(225) Argument to Moderation\n",
      "(226) Broken Window Fallacy\n",
      "(227) Self Sealing Argument\n",
      "(228) Unfalsifiability\n",
      "(229) Conspiracy Theory\n",
      "(230) Confusing an Explanation with an Excuse\n",
      "(231) Limited Depth\n",
      "(232) Alleged Certainty\n",
      "You should only answer the name of the fallacy.\n",
      "What type of fallacy does the following reasoning step belong to?\n",
      "[step]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_e21 = get_classification_prompt_template()\n",
    "print(prompt_template_e21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc275feb959a3213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:22:45.470931Z",
     "start_time": "2024-11-01T07:38:55.385146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 08:41:10] Processed 100 responses for LLM llama_3_1_70b (index=338).\n",
      "[2024-11-01 08:43:39] Processed 200 responses for LLM llama_3_1_70b (index=438).\n",
      "[2024-11-01 08:46:31] Processed 300 responses for LLM llama_3_1_70b (index=538).\n",
      "[2024-11-01 08:48:54] Processed 400 responses for LLM llama_3_1_70b (index=638).\n",
      "[2024-11-01 08:51:39] Processed 500 responses for LLM llama_3_1_70b (index=738).\n",
      "[2024-11-01 08:54:39] Processed 600 responses for LLM llama_3_1_70b (index=838).\n",
      "[2024-11-01 08:57:17] Processed 700 responses for LLM llama_3_1_70b (index=938).\n",
      "[2024-11-01 08:59:49] Processed 800 responses for LLM llama_3_1_70b (index=1038).\n",
      "[2024-11-01 09:02:41] Processed 900 responses for LLM llama_3_1_70b (index=1138).\n",
      "[2024-11-01 09:05:25] Processed 1000 responses for LLM llama_3_1_70b (index=1238).\n",
      "[2024-11-01 09:08:19] Processed 1100 responses for LLM llama_3_1_70b (index=1338).\n",
      "[2024-11-01 09:11:05] Processed 1200 responses for LLM llama_3_1_70b (index=1438).\n",
      "[2024-11-01 09:13:50] Processed 1300 responses for LLM llama_3_1_70b (index=1538).\n",
      "[2024-11-01 09:16:26] Processed 1400 responses for LLM llama_3_1_70b (index=1638).\n",
      "[2024-11-01 09:19:06] Processed 1500 responses for LLM llama_3_1_70b (index=1738).\n",
      "[2024-11-01 09:21:31] Processed 1600 responses for LLM llama_3_1_70b (index=1838).\n",
      "[2024-11-01 09:24:16] Processed 1700 responses for LLM llama_3_1_70b (index=1938).\n",
      "[2024-11-01 09:26:48] Processed 1800 responses for LLM llama_3_1_70b (index=2038).\n",
      "[2024-11-01 09:29:23] Processed 1900 responses for LLM llama_3_1_70b (index=2138).\n",
      "[2024-11-01 09:32:01] Processed 2000 responses for LLM llama_3_1_70b (index=2238).\n",
      "[2024-11-01 09:36:23] Processed 100 responses for LLM llama_3_1_8b (index=99).\n",
      "[2024-11-01 09:38:27] Processed 200 responses for LLM llama_3_1_8b (index=199).\n",
      "[2024-11-01 09:40:31] Processed 300 responses for LLM llama_3_1_8b (index=299).\n",
      "[2024-11-01 09:42:39] Processed 400 responses for LLM llama_3_1_8b (index=399).\n",
      "[2024-11-01 09:44:53] Processed 500 responses for LLM llama_3_1_8b (index=499).\n",
      "[2024-11-01 09:46:57] Processed 600 responses for LLM llama_3_1_8b (index=599).\n",
      "[2024-11-01 09:49:01] Processed 700 responses for LLM llama_3_1_8b (index=699).\n",
      "[2024-11-01 09:51:01] Processed 800 responses for LLM llama_3_1_8b (index=799).\n",
      "[2024-11-01 09:53:06] Processed 900 responses for LLM llama_3_1_8b (index=899).\n",
      "[2024-11-01 09:55:15] Processed 1000 responses for LLM llama_3_1_8b (index=999).\n",
      "[2024-11-01 09:57:20] Processed 1100 responses for LLM llama_3_1_8b (index=1099).\n",
      "[2024-11-01 09:59:27] Processed 1200 responses for LLM llama_3_1_8b (index=1199).\n",
      "[2024-11-01 10:01:30] Processed 1300 responses for LLM llama_3_1_8b (index=1299).\n",
      "[2024-11-01 10:03:36] Processed 1400 responses for LLM llama_3_1_8b (index=1399).\n",
      "[2024-11-01 10:05:40] Processed 1500 responses for LLM llama_3_1_8b (index=1499).\n",
      "[2024-11-01 10:07:46] Processed 1600 responses for LLM llama_3_1_8b (index=1599).\n",
      "[2024-11-01 10:09:49] Processed 1700 responses for LLM llama_3_1_8b (index=1699).\n",
      "[2024-11-01 10:11:55] Processed 1800 responses for LLM llama_3_1_8b (index=1799).\n",
      "[2024-11-01 10:13:59] Processed 1900 responses for LLM llama_3_1_8b (index=1899).\n",
      "[2024-11-01 10:16:04] Processed 2000 responses for LLM llama_3_1_8b (index=1999).\n",
      "[2024-11-01 10:18:11] Processed 2100 responses for LLM llama_3_1_8b (index=2099).\n",
      "[2024-11-01 10:20:14] Processed 2200 responses for LLM llama_3_1_8b (index=2199).\n",
      "[2024-11-01 10:22:19] Processed 2300 responses for LLM llama_3_1_8b (index=2299).\n"
     ]
    }
   ],
   "source": [
    "llms = get_llms([LLM.LLAMA_3_1_70B, LLM.LLAMA_3_1_8B])\n",
    "\n",
    "run_experiment(df_fallacies_e21, filename_21, prompt_template_e21, llms, sleep_seconds=0.1)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e21, filename_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fa42a73fc8e66",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Fallacy Classification with Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baec47ce0cd3a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T13:41:15.475569Z",
     "start_time": "2024-10-28T13:41:15.449984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-28 14:41:15] Loaded existing fallacy dataframe from data/fallacies_e22.csv.\n"
     ]
    }
   ],
   "source": [
    "filename_e22 = 'data/fallacies_e22.csv'\n",
    "df_fallacies_e22 = get_fallacy_df(filename_e22, only_incorrect=True)\n",
    "\n",
    "# Select only test set\n",
    "df_fallacies_e22 = df_fallacies_e22[df_fallacies_e22['tuning'] == TuningSet.TEST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9add2be6c27b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_e22 = get_classification_prompt_template()\n",
    "print(prompt_template_e22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfba5a381e2560bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T14:06:14.968149Z",
     "start_time": "2024-10-28T13:51:46.025181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-28 14:52:57] Processed 100 responses for LLM gpt_4o_mini_tuned_v1 (index=199).\n",
      "[2024-10-28 14:54:09] Processed 200 responses for LLM gpt_4o_mini_tuned_v1 (index=399).\n",
      "[2024-10-28 14:55:21] Processed 300 responses for LLM gpt_4o_mini_tuned_v1 (index=599).\n",
      "[2024-10-28 14:56:34] Processed 400 responses for LLM gpt_4o_mini_tuned_v1 (index=799).\n",
      "[2024-10-28 14:57:48] Processed 500 responses for LLM gpt_4o_mini_tuned_v1 (index=999).\n",
      "[2024-10-28 14:58:59] Processed 600 responses for LLM gpt_4o_mini_tuned_v1 (index=1199).\n",
      "[2024-10-28 15:00:17] Processed 700 responses for LLM gpt_4o_mini_tuned_v1 (index=1399).\n",
      "[2024-10-28 15:01:31] Processed 800 responses for LLM gpt_4o_mini_tuned_v1 (index=1599).\n",
      "[2024-10-28 15:02:52] Processed 900 responses for LLM gpt_4o_mini_tuned_v1 (index=1799).\n",
      "[2024-10-28 15:04:13] Processed 1000 responses for LLM gpt_4o_mini_tuned_v1 (index=1999).\n",
      "[2024-10-28 15:05:30] Processed 1100 responses for LLM gpt_4o_mini_tuned_v1 (index=2199).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llms = get_llms([LLM.GPT_4O_MINI_CLASSIFICATION])\n",
    "\n",
    "run_experiment(df_fallacies_e22, filename_e22, prompt_template_e22, llms, sleep_seconds=0.5)\n",
    "\n",
    "save_fallacy_df(df_fallacies_e22, filename_e22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb9039eae38931",
   "metadata": {},
   "source": [
    "## Fallacy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6f53e168f34f5",
   "metadata": {},
   "source": [
    "### Experiment 3.1: Fallacy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca026fe482a2ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T09:41:03.710058Z",
     "start_time": "2024-11-25T09:41:03.278652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-25 10:41:03] Loaded existing mafalda dataframe from data/mafalda_e31_v2.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentences_with_labels</th>\n",
       "      <th>gpt_4o_mini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TITLE: Endless Ledge Skip Campaign for Alts PO...</td>\n",
       "      <td>[[155, 588, slippery slope]]</td>\n",
       "      <td>['Slippery slope: P1 = poster, A = why not jus...</td>\n",
       "      <td>{'TITLE: Endless Ledge Skip Campaign for Alts ...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.SLIPP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two of my best friends are really introverted,...</td>\n",
       "      <td>[[84, 145, hasty generalization]]</td>\n",
       "      <td>[\"Based on two people only, you can't draw gen...</td>\n",
       "      <td>{'Two of my best friends are really introverte...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.HASTY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TITLE: There is a difference between a'smurf' ...</td>\n",
       "      <td>[[118, 265, false analogy]]</td>\n",
       "      <td>['False Analogy: X: Having an alt , Y: smurfin...</td>\n",
       "      <td>{'TITLE: There is a difference between a'smurf...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.FALSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TITLE: Discussion Thread (Part 3): 2020 Presid...</td>\n",
       "      <td>[[107, 261, guilt by association], [107, 338, ...</td>\n",
       "      <td>['Circular reasoning: X = The status quo in Am...</td>\n",
       "      <td>{'TITLE: Discussion Thread (Part 3): 2020 Pres...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.HASTY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America is the best place to live, because it'...</td>\n",
       "      <td>[[0, 78, circular reasoning]]</td>\n",
       "      <td>['Circular reasoning: X=America is the best pl...</td>\n",
       "      <td>{'America is the best place to live, because i...</td>\n",
       "      <td>fallacies=[FallacyEntry(fallacy=&lt;Fallacy.CIRCU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  TITLE: Endless Ledge Skip Campaign for Alts PO...   \n",
       "1  Two of my best friends are really introverted,...   \n",
       "2  TITLE: There is a difference between a'smurf' ...   \n",
       "3  TITLE: Discussion Thread (Part 3): 2020 Presid...   \n",
       "4  America is the best place to live, because it'...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       [[155, 588, slippery slope]]   \n",
       "1                  [[84, 145, hasty generalization]]   \n",
       "2                        [[118, 265, false analogy]]   \n",
       "3  [[107, 261, guilt by association], [107, 338, ...   \n",
       "4                      [[0, 78, circular reasoning]]   \n",
       "\n",
       "                                            comments  \\\n",
       "0  ['Slippery slope: P1 = poster, A = why not jus...   \n",
       "1  [\"Based on two people only, you can't draw gen...   \n",
       "2  ['False Analogy: X: Having an alt , Y: smurfin...   \n",
       "3  ['Circular reasoning: X = The status quo in Am...   \n",
       "4  ['Circular reasoning: X=America is the best pl...   \n",
       "\n",
       "                               sentences_with_labels  \\\n",
       "0  {'TITLE: Endless Ledge Skip Campaign for Alts ...   \n",
       "1  {'Two of my best friends are really introverte...   \n",
       "2  {'TITLE: There is a difference between a'smurf...   \n",
       "3  {'TITLE: Discussion Thread (Part 3): 2020 Pres...   \n",
       "4  {'America is the best place to live, because i...   \n",
       "\n",
       "                                gpt_4o_mini_response  \n",
       "0  fallacies=[FallacyEntry(fallacy=<Fallacy.SLIPP...  \n",
       "1  fallacies=[FallacyEntry(fallacy=<Fallacy.HASTY...  \n",
       "2  fallacies=[FallacyEntry(fallacy=<Fallacy.FALSE...  \n",
       "3  fallacies=[FallacyEntry(fallacy=<Fallacy.HASTY...  \n",
       "4  fallacies=[FallacyEntry(fallacy=<Fallacy.CIRCU...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_e31 = 'data/mafalda_e31_v2.csv'\n",
    "df_mafalda_e31 = get_mafalda_df(filename_e31)\n",
    "df_mafalda_e31.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca1ad5aaa36fda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T09:41:06.295288Z",
     "start_time": "2024-11-25T09:41:06.274506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at detecting and analyzing logical fallacies. Your task is to detect and analyze logical fallacies in the provided text.  \n",
      "\n",
      "Output Format:\n",
      "Provide your analysis in JSON format with the following structure for each identified fallacy:\n",
      "{\n",
      "  \"fallacies\": [\n",
      "    {\n",
      "      \"fallacy\": \"<fallacy_type>\",\n",
      "      \"span\": \"<text_span>\",\n",
      "      \"reason\": \"<reason>\",\n",
      "      \"defense\": \"<defense>\",\n",
      "      \"confidence\": <confidence>\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response Fields:\n",
      "1. <fallacy_type>: Only use fallacies from this approved list: Appeal to Anger, Appeal to Fear, Appeal to Pity, Appeal to Positive Emotion, Appeal to Ridicule, Appeal to Worse Problems, Causal Oversimplification, Circular Reasoning, Equivocation, Fallacy of Division, False Analogy, False Causality, False Dilemma, Hasty Generalization, Slippery Slope, Strawman Fallacy, Ad Hominem, Ad Populum, Appeal to Authority, Appeal to Nature, Appeal to Tradition, Guilt by Association, Tu Quoque\n",
      "2. <text_span>:\n",
      "   - Include the complete context needed to understand the fallacy, but keep the span as short as possible\n",
      "   - Can overlap with other identified fallacies\n",
      "   - Must be verbatim quotes from the original text\n",
      "3. <reason>:\n",
      "   - Provide clear, specific explanations\n",
      "   - Include both why it qualifies as a fallacy and how it violates logical reasoning\n",
      "4. <defense>:\n",
      "   - Provide the strongest possible charitable interpretation under the assumption that the argument is valid or reasonable, and not a fallacy\n",
      "   - Consider implicit premises that could validate the argument\n",
      "5. <confidence>: Rate your confidence in each fallacy identification from 0.0 to 1.0, taking into account the reasoning and defense\n",
      "\n",
      "Guidelines:\n",
      "- Apply the principle of charity, consider the argument in its strongest form, and avoid over-detection\n",
      "- Consider principles of formal logical reasoning when judging the validity of an argument\n",
      "- For formal logical arguments, accept premises as true for the sake of the argument\n",
      "- Return an empty list if no clear logical fallacies are present\n",
      "- Adjust confidence scores downward in proportion to the strength and plausibility of the defense\n",
      "- Consider context and implicit assumptions\n",
      "- Think step by step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt_e31 = get_mafalda_search_system_prompt_v2()\n",
    "print(system_prompt_e31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27276a85b705430e",
   "metadata": {},
   "source": [
    "This system prompt has been improved iteratively, based on the MAFALDA F1 score and partial inspection of the\n",
    "response quality by the author.\n",
    "\n",
    "- The explicit JSON format is probably not needed, the OpenAI's structured output works without it.\n",
    "- The contradiction between thoroughness and principle of charity is a challenge.\n",
    "- The confidence score can be used to filter out questionable fallacy detections. Observed scores are in the range 0.5-0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae494eeda5523c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T09:55:01.722734Z",
     "start_time": "2024-11-25T09:41:42.660233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-25 10:48:35] Processed 100 responses for LLM gpt_4o (index=99).\n",
      "[2024-11-25 10:55:01] Processed 200 responses for LLM gpt_4o (index=199).\n"
     ]
    }
   ],
   "source": [
    "llms = get_fallacy_search_llms([LLM.GPT_4O])\n",
    "\n",
    "run_experiment(df_mafalda_e31, filename_e31, system_prompt_e31, llms, is_search=True)\n",
    "\n",
    "save_mafalda_df(df_mafalda_e31, filename_e31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7426f9bf5cba4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
